interactions:
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/3e669fbc-7165-44e5-8b66-060fd4dc4c75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\nworkflow test_nonstandard_outputs
        {\n    call generate_diverse_outputs\n    \n    output {\n        File special_chars
        = generate_diverse_outputs.file_special_chars\n        File no_extension =
        generate_diverse_outputs.file_no_extension\n        File nested_output = generate_diverse_outputs.nested_file\n        File
        symlink_file = generate_diverse_outputs.symlink_output\n        Array[File]
        glob_files = generate_diverse_outputs.pattern_files\n    }\n}\n\ntask generate_diverse_outputs
        {\n    command <<<\n        # File with special characters\n        echo \"test
        content\" > \"test@file#1.txt\"\n        \n        # File without extension\n        echo
        \"no extension\" > datafile\n        \n        # Nested directory output\n        mkdir
        -p nested/dir\n        echo \"nested content\" > nested/dir/test.txt\n        \n        #
        Create a symlink\n        echo \"original\" > original.txt\n        ln -s
        original.txt link.txt\n        \n        # Multiple pattern files\n        for
        i in {1..3}; do\n            echo \"pattern $i\" > \"pattern_$i.out\"\n        done\n    >>>\n\n    output
        {\n        File file_special_chars = \"test@file#1.txt\"\n        File file_no_extension
        = \"datafile\"\n        File nested_file = \"nested/dir/test.txt\"\n        File
        symlink_output = \"link.txt\"\n        Array[File] pattern_files = glob(\"pattern_*.out\")\n    }\n\n    runtime
        {\n        docker: \"ubuntu:noble-20241118.1\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"3e669fbc-7165-44e5-8b66-060fd4dc4c75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-3e669fbc-7165-44e5-8b66-060fd4dc4c75"},"submission":"2025-02-07T01:05:57.226Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:05:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1899'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/6b021266-f2d5-4cfe-a3b9-a370f9bc2ae5/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n## This is a test workflow
        that returns the hostname of the node \n## the job is submitted to as a test
        for module functionality on Gizmo.\n## Added verification of module loading.\n\n####
        WORKFLOW DEFINITION\n\nworkflow HelloModuleHostname {\n  input {\n    String
        module2load = \"Python/3.12.3-GCCcore-13.3.0\" # Default value can be overwritten\n  }\n\n  call
        Hostname {\n    input:\n      module_env = module2load\n  }\n\n  output {\n    File
        stdout = Hostname.out\n    Boolean module_loaded = Hostname.module_verified\n  }\n\n  parameter_meta
        {\n    stdout: \"hostname of the node the job was submitted to\"\n    module_loaded:
        \"boolean indicating if the specified module was successfully loaded\"\n  }\n}\n\n####
        TASK DEFINITIONS\n\ntask Hostname {\n  input {\n    String module_env\n  }\n\n  command
        <<<\n    set -e  # Exit on any error\n    \n    # Get hostname\n    hostname\n    \n    #
        List loaded modules and verify our module is loaded\n    source /app/lmod/lmod/init/profile\n    module
        list 2>&1 | tee module_list.txt\n    \n    # Check if the module or its base
        name is in the loaded modules\n    if grep -q \"~{module_env}\" module_list.txt;
        then\n      echo \"true\" > module_verified.txt\n      echo \"Successfully
        verified module ~{module_env} is loaded\"\n    else\n      echo \"ERROR: Module
        ~{module_env} was not found in loaded modules:\"\n      cat module_list.txt\n      exit
        1\n    fi\n  >>>\n\n  output {\n    File out = stdout()\n    Boolean module_verified
        = read_boolean(\"module_verified.txt\")\n  }\n\n  runtime {\n    cpu: 1\n    memory:
        \"1 GB\"\n    modules: \"~{module_env}\"\n  }\n\n  parameter_meta {\n    module_env:
        \"name of modules to be loaded\"\n    out: \"output file containing hostname
        and module information\"\n    module_verified: \"boolean indicating if the
        requested module was successfully loaded\"\n  }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"6b021266-f2d5-4cfe-a3b9-a370f9bc2ae5","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-6b021266-f2d5-4cfe-a3b9-a370f9bc2ae5"},"submission":"2025-02-07T01:05:57.260Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:05:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '2321'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/4c0126b3-f9e8-4fa8-a0b0-0e6a9f2d5ec1/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\nworkflow ArrayOperations
        {\n    input {\n        # Input arrays for different tests\n        Array[String]
        strings\n        Array[String] additional_strings = []  # For testing array
        concatenation\n        Array[Array[String]] nested_arrays = []  # For testing
        nested arrays\n        Array[Int] numbers = [1, 2, 3, 4, 5]  # Default integer
        array for numeric operations\n        Array[File] input_files = [] # Array
        of files to test file operations\n    }\n    \n    # Scatter operation to
        test processing of each element in an array\n    # Test empty arrays (original
        operation still works with empty input)\n    scatter (str in strings) {\n        call
        Uppercase { input: text = str }\n    }\n    \n    # Test array indexing (accessing
        first and last elements)\n    if (length(strings) > 0) {\n        call ValidateIndex
        { input: arr = strings }\n    }\n    \n    # Test array functions like sorting,
        length calculation, and flattening\n    call ArrayFunctions { \n        input:
        \n            arr = strings,\n            nested = nested_arrays\n    }\n    \n    #
        Test array concatenation and verify the combined length\n    Array[String]
        combined = flatten([strings, additional_strings])\n    call ArrayConcat {\n        input:
        \n            arr1 = strings,\n            arr2 = additional_strings,\n            expected_length
        = length(combined)\n    }\n    \n    # Test integer array operations like
        summation and combining arrays\n    Array[Int] more_numbers = [6, 7, 8, 9,
        10]  # Intermediate array declaration\n    call IntegerArrayOps {\n        input:\n            numbers
        = numbers,\n            additional_numbers = more_numbers\n    }\n\n    #
        Test file array operations like localization and content reading\n    if (length(input_files)
        > 0) {\n        call FileArrayOps {\n            input:\n                files
        = input_files\n        }\n    }\n    # Outputs to capture results of the tests\n    output
        {\n        Array[String] uppercased = Uppercase.out # Outputs from scatter
        task\n        Int? first_index = ValidateIndex.first_index # First index in
        string array\n        Int? last_index = ValidateIndex.last_index # Last index
        in string array\n        Array[String] sorted_array = ArrayFunctions.sorted
        # Sorted array\n        Array[Array[String]] processed_nested = ArrayFunctions.processed_nested
        # Processed nested array\n        Boolean concat_test_passed = ArrayConcat.test_passed
        # Result of concatenation test\n        Int array_length = ArrayFunctions.arr_length
        # Length of input array\n        Array[String] flattened = ArrayFunctions.flattened
        # Flattened nested arrays\n        # New outputs for integer array operations
        \n        Int sum_result = IntegerArrayOps.sum # Sum of integer array\n        Array[Int]
        combined_numbers = IntegerArrayOps.combined # Combined integer arrays\n        #
        New outputs for file array operations\n        Array[String]? file_contents
        = FileArrayOps.contents # Contents of files\n        Boolean? files_localized
        = FileArrayOps.localization_success # File localization status\n    }\n\n    parameter_meta
        {\n        # Descriptions for inputs\n        strings: \"Primary array of
        input strings\"\n        additional_strings: \"Secondary array for testing
        concatenation\"\n        nested_arrays: \"Array of arrays for testing nested
        array operations\"\n        numbers: \"Array of integers for testing numeric
        operations\"\n        input_files: \"Array of input files for testing file
        localization\"\n    }\n}\n\n# Task to convert string to uppercase (tests per-element
        processing)\ntask Uppercase {\n    input {\n        String text\n    }\n    \n    command
        <<<\n        echo \"~{text}\" | tr ''[:lower:]'' ''[:upper:]''\n    >>>\n    \n    output
        {\n        String out = read_string(stdout())\n    }\n    \n    runtime {\n        cpu:
        1\n        memory: \"1 GB\"\n    }\n}\n\n\n# Task to test indexing operations\ntask
        ValidateIndex {\n    input {\n        Array[String] arr\n    }\n    \n    command
        <<<\n        echo \"0\" > first_index.txt  # First index\n        echo \"~{length(arr)-1}\"
        > last_index.txt  # Last index\n    >>>\n    \n    output {\n        Int first_index
        = read_int(\"first_index.txt\")\n        Int last_index = read_int(\"last_index.txt\")\n    }\n    \n    runtime
        {\n        cpu: 1\n        memory: \"1 GB\"\n    }\n}\n\n# Task to test array
        functions\ntask ArrayFunctions {\n    input {\n        Array[String] arr\n        Array[Array[String]]
        nested\n    }\n    \n    command <<<\n        # Sort the input array using
        bash\n        echo \"~{sep=''\\n'' arr}\" | sort > sorted.txt\n        \n        #
        Get array length\n        echo \"~{length(arr)}\" > length.txt\n        \n        #
        Process nested arrays (flatten them)\n        echo \"~{sep=''\\n'' flatten(nested)}\"
        > flattened.txt\n    >>>\n    \n    output {\n        Array[String] sorted
        = read_lines(\"sorted.txt\")\n        Int arr_length = read_int(\"length.txt\")\n        Array[String]
        flattened = read_lines(\"flattened.txt\")\n        Array[Array[String]] processed_nested
        = nested  # Return the original nested array\n    }\n    \n    runtime {\n        cpu:
        1\n        memory: \"1 GB\"\n    }\n}\n\n# Task to test concatenation of two
        arrays\ntask ArrayConcat {\n    input {\n        Array[String] arr1\n        Array[String]
        arr2\n        Int expected_length\n    }\n    \n    command <<<\n        actual_length=$((
        ~{length(arr1)} + ~{length(arr2)} ))\n        if [ \"$actual_length\" -eq
        ~{expected_length} ]; then\n            echo \"true\"\n        else\n            echo
        \"false\"\n        fi\n    >>>\n    \n    output {\n        Boolean test_passed
        = read_boolean(stdout())\n    }\n    \n    runtime {\n        cpu: 1\n        memory:
        \"1 GB\"\n    }\n}\n\n# Task to test integer array operations\ntask IntegerArrayOps
        {\n    input {\n        Array[Int] numbers\n        Array[Int] additional_numbers\n    }\n    \n    command
        <<<\n        # Calculate sum of numbers to verify proper parsing\n        total=0\n        for
        num in ~{sep='' '' numbers}; do\n            total=$((total + num))\n        done\n        echo
        $total > sum.txt\n\n        # Combine arrays and write to file\n        echo
        \"~{sep=''\\n'' flatten([numbers, additional_numbers])}\" > combined.txt\n    >>>\n    \n    output
        {\n        Int sum = read_int(\"sum.txt\")\n        Array[Int] combined =
        read_lines(\"combined.txt\")\n    }\n    \n    runtime {\n        cpu: 1\n        memory:
        \"1 GB\"\n    }\n}\n\n# Task to test file array operations\ntask FileArrayOps
        {\n    input {\n        Array[File] files\n    }\n    \n    command <<<\n        #
        Test file localization by reading contents\n        for file in ~{sep='' ''
        files}; do\n            if [ -f \"$file\" ]; then\n                cat \"$file\"
        >> all_contents.txt\n                echo \"---\" >> all_contents.txt  # Separator
        between files\n            else\n                echo \"false\" > localization_success.txt\n                exit
        1\n            fi\n        done\n        echo \"true\" > localization_success.txt\n    >>>\n    \n    output
        {\n        Array[String] contents = read_lines(\"all_contents.txt\")\n        Boolean
        localization_success = read_boolean(\"localization_success.txt\")\n    }\n    \n    runtime
        {\n        cpu: 1\n        memory: \"1 GB\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"4c0126b3-f9e8-4fa8-a0b0-0e6a9f2d5ec1","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-4c0126b3-f9e8-4fa8-a0b0-0e6a9f2d5ec1"},"submission":"2025-02-07T01:05:57.308Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:05:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '7734'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/a0aa6c51-9779-4231-9c1b-bd2158d4a26e/metadata?expandSubWorkflows=true
  response:
    body:
      string: "{\n  \"status\": \"fail\",\n  \"message\": \"Unrecognized workflow
        ID: a0aa6c51-9779-4231-9c1b-bd2158d4a26e\"\n}"
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:05:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '101'
    status:
      code: 404
      message: Not Found
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/a0aa6c51-9779-4231-9c1b-bd2158d4a26e/metadata?expandSubWorkflows=true
  response:
    body:
      string: "{\n  \"status\": \"fail\",\n  \"message\": \"Unrecognized workflow
        ID: a0aa6c51-9779-4231-9c1b-bd2158d4a26e\"\n}"
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:06:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '101'
    status:
      code: 404
      message: Not Found
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/a0aa6c51-9779-4231-9c1b-bd2158d4a26e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\nworkflow basicGlobTest
        {\n    call create_files\n    output {\n        Array[File] matched_files
        = create_files.txt_files\n    }\n}\n\ntask create_files {\n    command <<<\n        echo
        \"File 1\" > file1.txt\n        echo \"File 2\" > file2.txt\n    >>>\n    output
        {\n        Array[File] txt_files = glob(\"*.txt\")\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a0aa6c51-9779-4231-9c1b-bd2158d4a26e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a0aa6c51-9779-4231-9c1b-bd2158d4a26e"},"submission":"2025-02-07T01:05:57.343Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:06:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '790'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/c94fab6c-9741-4d4b-a805-b7ea23816325/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n## This is a test workflow
        that returns the hostname of the node \n## the job is submitted to as a test
        for the Gizmo backend. \n\n#### WORKFLOW DEFINITION\n\nworkflow HelloHostname
        {\n  call Hostname {\n  }\n\n  output {\n    File stdout = Hostname.out\n  }\n\n  parameter_meta
        {\n    stdout: \"hostname of the node the job was submitted to\"\n  }\n}\n\n####
        TASK DEFINITIONS\n\ntask Hostname {\n  command <<<\n    echo $(hostname)\n  >>>\n\n  output
        {\n    File out = stdout()\n  }\n  \n  runtime {\n    cpu: 1\n    memory:
        \"1 GB\"\n  }\n\n  parameter_meta {\n    out: \"hostname of the node the job
        was submitted to\"\n  }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"c94fab6c-9741-4d4b-a805-b7ea23816325","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-c94fab6c-9741-4d4b-a805-b7ea23816325"},"submission":"2025-02-07T01:05:57.384Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:06:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1090'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/46a6bd85-f11f-42c5-b1e5-a1056c1d315d/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n## This workflow demonstrates
        the usage of conditional statements in WDL\n## by selectively processing samples
        based on their properties\n\nstruct SampleInfo {\n    String name\n    Float
        quality_score\n    String type\n}\n\nworkflow conditional_example {\n    input
        {\n        Array[SampleInfo] samples\n        Float quality_threshold\n    }\n\n    #
        Demonstrate if statement in scatter\n    scatter (sample in samples) {\n        if
        (sample.quality_score >= quality_threshold) {\n            call process_high_quality
        {\n                input:\n                    sample = sample\n            }\n        }\n    }\n\n    #
        Create string arrays for the QC report\n    scatter (sample in samples) {\n        String
        sample_line = \"~{sample.name},~{sample.type},~{sample.quality_score}\"\n    }\n\n    #
        Demonstrate single conditional task\n    call run_qc_report {\n        input:\n            sample_lines
        = sample_line\n    }\n\n    # Calculate number of high quality samples\n    Int
        num_high_quality = length(select_all(process_high_quality.message))\n\n    #
        Demonstrate separate conditional blocks (WDL 1.0 approach instead of if/else)\n    Boolean
        has_multiple_samples = num_high_quality > 1\n    \n    if (has_multiple_samples)
        {\n        call summarize {\n            input:\n                messages
        = select_all(process_high_quality.message),\n                report = \"Multiple
        high-quality samples processed\"\n        }\n    }\n\n    if (!has_multiple_samples)
        {\n        call summarize as summarize_few {\n            input:\n                messages
        = select_all(process_high_quality.message),\n                report = \"Few
        or no high-quality samples found\"\n        }\n    }\n\n    output {\n        String
        final_summary = select_first([summarize.summary, summarize_few.summary])\n        File
        qc_report = run_qc_report.report_csv\n    }\n}\n\ntask process_high_quality
        {\n    input {\n        SampleInfo sample\n    }\n\n    command <<<\n        echo
        \"Processing high quality ~{sample.type} sample ~{sample.name} (Q=~{sample.quality_score})\"\n    >>>\n\n    output
        {\n        String message = read_string(stdout())\n    }\n\n    runtime {\n        docker:
        \"ubuntu:noble-20241118.1\"\n    }\n}\n\ntask run_qc_report {\n    input {\n        Array[String]
        sample_lines\n    }\n\n    command <<<\n        echo \"Quality Score Summary:\"\n        echo
        \"Sample Name,Type,Quality Score\" > report.csv\n        ~{sep=\"\\n\" sample_lines}
        >> report.csv\n        cat report.csv\n    >>>\n\n    output {\n        String
        report = read_string(stdout())\n        File report_csv = \"report.csv\"\n    }\n\n    runtime
        {\n        docker: \"ubuntu:noble-20241118.1\"\n    }\n}\n\ntask summarize
        {\n    input {\n        Array[String] messages\n        String report\n    }\n\n    command
        <<<\n        echo \"~{report}\"\n        echo \"Number of samples processed:
        ~{length(messages)}\"\n    >>>\n\n    output {\n        String summary = read_string(stdout())\n    }\n\n    runtime
        {\n        docker: \"ubuntu:noble-20241118.1\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"46a6bd85-f11f-42c5-b1e5-a1056c1d315d","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-46a6bd85-f11f-42c5-b1e5-a1056c1d315d"},"submission":"2025-02-07T01:05:57.429Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:06:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3510'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/14df1406-6d8a-4d1c-adba-ef4cec0acfc0/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"14df1406-6d8a-4d1c-adba-ef4cec0acfc0","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-14df1406-6d8a-4d1c-adba-ef4cec0acfc0"},"submission":"2025-02-07T01:05:57.478Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:06:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1700'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/ef81e2c3-1200-41ac-a941-1cb18a4435a0/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n## This is a test workflow
        that fails against womtool.\n## From https://github.com/broadinstitute/cromwell\n\n####
        WORKFLOW DEFINITION\n\nworkflow oops {\n  call oopsie\n}\n\n#### TASK DEFINITIONS\n\ntask
        oopsie {\n  input {\n    String str\n  }\n  command { echo ${str} }\n  runtime
        { docker: docker_image }\n}\n","root":"","options":"{\n\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"ef81e2c3-1200-41ac-a941-1cb18a4435a0","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-ef81e2c3-1200-41ac-a941-1cb18a4435a0"},"submission":"2025-02-07T01:05:57.520Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:06:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '657'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/9b0fe028-641b-45cf-8118-6cc0d684d7da/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\nworkflow emptyGlobTest
        {\n    call create_empty_directory\n\n    output {\n        Array[File] no_files
        = create_empty_directory.no_files\n    }\n}\n\ntask create_empty_directory
        {\n    command {\n        mkdir empty_dir\n    }\n    output {\n        Array[File]
        no_files = glob(\"empty_dir/*.txt\")\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"9b0fe028-641b-45cf-8118-6cc0d684d7da","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-9b0fe028-641b-45cf-8118-6cc0d684d7da"},"submission":"2025-02-07T01:05:57.560Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:06:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '772'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/b29e5f84-7cd0-4d24-864b-53b314371f25/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\nworkflow WildcardsandConditions
        {\n    input {\n        String prefix  # Required input for the file prefix
        (no default value)\n    }\n\n    call wildcard_and_conditions_test {\n        input:\n            prefix
        = prefix  # Explicitly pass the workflow input to the task\n    }\n\n    output
        {\n        Array[File] txt_files = wildcard_and_conditions_test.txt_files\n        String
        conditional_result = wildcard_and_conditions_test.conditional_output\n    }\n}\n\ntask
        wildcard_and_conditions_test {\n    input {\n        String prefix  # Required
        input for file creation\n        Boolean create_extra_file = true  # Default
        value for conditional logic\n    }\n\n    command <<<\n        # Create multiple
        .txt files to test wildcard resolution\n        for i in {1..3}; do\n            echo
        \"File content $i\" > \"~{prefix}_$i.txt\"\n        done\n\n        # Create
        an extra file conditionally\n        if [[ ~{create_extra_file} == \"true\"
        ]]; then\n            echo \"Extra file content\" > ~{prefix}_extra.txt\n        fi\n\n        #
        Parse inputs directly in the command\n        echo \"Parsed prefix: ~{prefix}\"
        > parsed_output.txt\n    >>>\n\n    output {\n        Array[File] txt_files
        = glob(\"*.txt\")  # Test wildcard resolution\n        String conditional_output
        = read_string(\"parsed_output.txt\")  # Verify input parsing\n    }\n\n    runtime
        {\n        docker: \"ubuntu:20.04\"\n    }\n}\n","root":"","options":"{\n\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"b29e5f84-7cd0-4d24-864b-53b314371f25","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-b29e5f84-7cd0-4d24-864b-53b314371f25"},"submission":"2025-02-07T01:05:57.612Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:06:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1756'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/ac74f017-653d-4f20-997d-81cdab20be9c/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\nworkflow testFileoperations
        {\n    call file_operations\n\n    output {\n        File created_file1 =
        file_operations.created_file1\n        File moved_file = file_operations.moved_file\n        File
        renamed_file = file_operations.renamed_file\n    }\n}\n\ntask file_operations
        {\n    command <<<\n        # Create three different files\n        echo \"This
        is the first created file.\" > file1.txt\n        echo \"This is the second
        file that will be moved.\" > file2.txt\n        echo \"This is the third file
        that will be renamed.\" > file3.txt\n        \n        # Move the second file
        to a new directory\n        mkdir -p output_dir\n        mv file2.txt output_dir/\n        \n        #
        Rename the third file\n        mv file3.txt file3_renamed.txt\n    >>>\n\n    output
        {\n        # Output the actual existing files\n        File created_file1
        = \"file1.txt\"                  # The first file remains unchanged\n        File
        moved_file = \"output_dir/file2.txt\"          # The second file after being
        moved\n        File renamed_file = \"file3_renamed.txt\"           # The third
        file after being renamed\n    }\n\n    runtime {\n        docker: \"ubuntu:20.04\"\n    }\n}\n","root":"","options":"{\n\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"ac74f017-653d-4f20-997d-81cdab20be9c","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-ac74f017-653d-4f20-997d-81cdab20be9c"},"submission":"2025-02-07T01:05:57.650Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:06:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1530'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/d43608a7-abde-4039-bae9-fe3c2d56308b/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# The basicTaskTest workflow
        calls a task named simpleTask, which takes a string input and writes it to
        a file called output.txt. It demonstrates a basic execution of a task with
        file output.\n\n# This tests basic task execution, input handling, and file
        output functionality. It ensures that a task can successfully take an input
        and generate an output.\n\nworkflow basicTaskTest {\n  input {\n    String
        text = \"Hello, World!\"\n  }\n\n  call simpleTask {\n    input:\n      message
        = text\n  }\n}\n\ntask simpleTask {\n  input {\n    String message\n  }\n\n  command
        <<<\n    echo \"${message}\" > output.txt\n    >>>\n\n  output {\n    File
        outputFile = \"output.txt\"\n  }\n\n  runtime {\n    docker: \"ubuntu:20.04\"\n  }\n}\n","root":"","options":"{\n\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"d43608a7-abde-4039-bae9-fe3c2d56308b","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-d43608a7-abde-4039-bae9-fe3c2d56308b"},"submission":"2025-02-07T01:05:57.688Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:06:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1079'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj32.fhcrc.org:35541
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj32.fhcrc.org:35541/api/workflows/v1/82a61df8-483d-450c-9b4a-26bb55ac1b37/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n## This is a test workflow
        that returns the Docker image name and tag\n## and measures execution time
        of the Hostname task.\n\n#### WORKFLOW DEFINITION\n\nworkflow HelloDockerHostname
        {\n  input {\n    String docker_image = \"ubuntu:20.04\"  # Default value
        but can be overridden\n  }\n\n  call GetStartTime\n\n  call Hostname {\n    input:\n      expected_image
        = docker_image,\n      start_time = GetStartTime.timestamp  # Add dependency
        on start time\n  }\n\n  call GetEndTime {\n    input:\n      hostname_done
        = Hostname.out  # Add dependency on Hostname completion\n  }\n\n  call ValidateExecutionTime
        {\n    input:\n      start_time = GetStartTime.timestamp,\n      end_time
        = GetEndTime.timestamp\n  }\n\n  output {\n    File stdout = Hostname.out\n    Float
        execution_time_seconds = ValidateExecutionTime.duration_seconds\n    Boolean
        within_time_limit = ValidateExecutionTime.within_limit\n  }\n\n  parameter_meta
        {\n    docker_image: \"Docker image to run the task in (e.g. ubuntu:latest)\"\n  }\n}\n\n####
        TASK DEFINITIONS\n\ntask GetStartTime {\n  command <<<\n    date +%s.%N\n  >>>\n\n  output
        {\n    Float timestamp = read_float(stdout())\n  }\n\n  runtime {\n    docker:
        \"ubuntu:20.04\"\n    cpu: 1\n    memory: \"1 GB\"\n  }\n}\n\ntask GetEndTime
        {\n  input {\n    File hostname_done  # Add dependency on Hostname completion\n  }\n\n  command
        <<<\n    date +%s.%N\n  >>>\n\n  output {\n    Float timestamp = read_float(stdout())\n  }\n\n  runtime
        {\n    docker: \"ubuntu:20.04\"\n    cpu: 1\n    memory: \"1 GB\"\n  }\n}\n\ntask
        ValidateExecutionTime {\n  input {\n    Float start_time\n    Float end_time\n  }\n\n  command
        <<<\n    # Calculate duration using awk for floating point arithmetic\n    duration=$(awk
        \"BEGIN {print ~{end_time} - ~{start_time}}\")\n    echo \"$duration\" > duration.txt\n    \n    #
        Check if duration is less than 120 seconds (2 minutes)\n    awk -v dur=\"$duration\"
        ''BEGIN {if (dur < 120) exit 0; exit 1}''\n    if [ $? -eq 0 ]; then\n      echo
        \"true\" > within_limit.txt\n    else\n      echo \"false\" > within_limit.txt\n    fi\n  >>>\n\n  output
        {\n    Float duration_seconds = read_float(\"duration.txt\")\n    Boolean
        within_limit = read_boolean(\"within_limit.txt\")\n  }\n\n  runtime {\n    docker:
        \"ubuntu:20.04\"\n    cpu: 1\n    memory: \"1 GB\"\n  }\n}\n\ntask Hostname
        {\n  input {\n    String expected_image\n    Float start_time  # Add start_time
        as input to create dependency\n  }\n\n  command <<<\n    # Split expected
        image into name and tag\n    EXPECTED_IMAGE_NAME=$(echo \"~{expected_image}\"
        | cut -d'':'' -f1)\n    EXPECTED_TAG=$(echo \"~{expected_image}\" | cut -d'':''
        -f2)\n\n    # Get current image info\n    CURRENT_IMAGE=$(grep \"ID=\" /etc/os-release
        | head -n1 | cut -d''='' -f2)\n    CURRENT_VERSION=$(grep \"VERSION_ID=\"
        /etc/os-release | cut -d''\"'' -f2)\n\n    # Compare image name\n    if [[
        \"$CURRENT_IMAGE\" != \"$EXPECTED_IMAGE_NAME\" ]]; then\n      echo \"Error:
        Expected Docker image $EXPECTED_IMAGE_NAME but got: $CURRENT_IMAGE\"\n      exit
        1\n    fi\n\n    # Compare version/tag\n    if [[ \"$CURRENT_VERSION\" !=
        \"$EXPECTED_TAG\" ]]; then\n      echo \"Error: Expected version $EXPECTED_TAG
        but got: $CURRENT_VERSION\"\n      exit 1\n    fi\n\n    echo \"Verified Docker
        Image: $CURRENT_IMAGE:$CURRENT_VERSION\"\n    echo \"Expected Image: ~{expected_image}\"\n    echo
        \"Hostname: $(hostname)\"\n  >>>\n\n  output {\n    File out = stdout()\n  }\n\n  runtime
        {\n    cpu: 1\n    memory: \"1 GB\"\n    docker: \"~{expected_image}\"\n  }\n\n  parameter_meta
        {\n    expected_image: \"Docker image that should be running this task\"\n  }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"82a61df8-483d-450c-9b4a-26bb55ac1b37","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-82a61df8-483d-450c-9b4a-26bb55ac1b37"},"submission":"2025-02-07T01:05:57.725Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 07 Feb 2025 01:06:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '4092'
    status:
      code: 200
      message: OK
version: 1
