interactions:
- request:
    body: "--66d6a56c83b2e510cd03839de660636d\r\nContent-Disposition: form-data; name=\"workflowSource\";
      filename=\"gpuAlloc.wdl\"\r\nContent-Type: application/octet-stream\r\n\r\nversion
      1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
      TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
      2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication works
      correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File
      test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n
      \       Array[Float] matrix_result = GpuTest.multiplication_result\n    }\n\n
      \   parameter_meta {\n        test_results: \"Complete log of the GPU test including
      tensor operations and GPU detection\"\n        gpu_count: \"Number of GPUs detected
      by TensorFlow\"\n        matrix_result: \"Results of the matrix multiplication
      operation\"\n    }\n}\n\ntask GpuTest {\n    command <<<\n        python3 <<CODE\n
      \       import tensorflow as tf\n        import numpy as np\n\n        # Test
      GPU availability\n        gpus = tf.config.experimental.list_physical_devices('GPU')\n
      \       detected_gpus = len(gpus)\n        print(f\"Number of GPUs detected:
      {detected_gpus}\")\n\n        # Verify GPU allocation matches runtime specification\n
      \       expected_gpus = 1  # Matches the runtime.gpus specification\n        if
      detected_gpus != expected_gpus:\n            raise RuntimeError(f\"GPU allocation
      mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n
      \       # Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0,
      4.0, 5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0,
      4.0, 5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n
      \       result = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n
      \       print(\"\\nMatrix A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix
      B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix Multiplication
      Result:\")\n        print(result.numpy())\n        \n        # Save results
      for output\n        np.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\n
      \       with open(\"gpu_count.txt\", \"w\") as f:\n            f.write(str(len(gpus)))\n
      \       CODE\n    >>>\n\n    output {\n        File results = stdout()\n        Int
      detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float] multiplication_result
      = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime {\n        docker:
      \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules: \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n
      \       gpus: \"1\"\n    }\n\n    parameter_meta {\n        results: \"Output
      log containing GPU detection and matrix multiplication results\"\n        detected_gpus:
      \"Number of GPUs detected by TensorFlow\"\n        multiplication_result: \"Flattened
      array containing the result of matrix multiplication\"\n    }\n}\n\r\n--66d6a56c83b2e510cd03839de660636d\r\nContent-Disposition:
      form-data; name=\"workflowOptions\"; filename=\"options.json\"\r\nContent-Type:
      application/json\r\n\r\n{\n    \"workflow_failure_mode\": \"ContinueWhilePossible\",\n
      \   \"write_to_cache\": false,\n    \"read_from_cache\": false\n}\n\r\n--66d6a56c83b2e510cd03839de660636d--\r\n"
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '3160'
      content-type:
      - multipart/form-data; boundary=66d6a56c83b2e510cd03839de660636d
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1
  response:
    body:
      string: '{"id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","status":"Submitted"}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '66'
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:33:10 GMT
      Server:
      - nginx/1.25.3
    status:
      code: 201
      message: Created
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:33:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:33:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:33:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:33:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:33:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:33:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:33:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:33:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"QueuedInCromwell","shardIndex":-1,"backend":"gizmo","attempt":1,"start":"2025-08-24T03:33:51.589Z"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:33:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3807'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"QueuedInCromwell","shardIndex":-1,"backend":"gizmo","attempt":1,"start":"2025-08-24T03:33:51.589Z"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:34:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3807'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","compressedDockerSize":"2871239351","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"backend":"gizmo","attempt":1,"start":"2025-08-24T03:33:51.589Z","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:34:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5682'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"29795065","backend":"gizmo","attempt":1,"start":"2025-08-24T03:33:51.589Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:34:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"29795065","backend":"gizmo","attempt":1,"start":"2025-08-24T03:33:51.589Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:34:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"29795065","backend":"gizmo","attempt":1,"start":"2025-08-24T03:33:51.589Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:34:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"29795065","backend":"gizmo","attempt":1,"start":"2025-08-24T03:33:51.589Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:34:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"29795065","backend":"gizmo","attempt":1,"start":"2025-08-24T03:33:51.589Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:34:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"29795065","backend":"gizmo","attempt":1,"start":"2025-08-24T03:33:51.589Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:34:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"29795065","backend":"gizmo","attempt":1,"start":"2025-08-24T03:33:51.589Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:34:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"29795065","backend":"gizmo","attempt":1,"start":"2025-08-24T03:33:51.589Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:34:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok20.fhcrc.org:44823
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok20.fhcrc.org:44823/api/workflows/v1/a83bf8ab-3ff4-4695-b174-6520a23b3d75/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-cc7e86b","description":"Finished","timestamp":"2025-08-24T03:34:47.690Z","cromwellVersion":"87"},{"cromwellId":"cromid-cc7e86b","description":"PickedUp","timestamp":"2025-08-24T03:33:50.518Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Done","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"outputs":{"multiplication_result":[22.0,28.0,49.0,64.0],"results":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","detected_gpus":1},"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"returnCode":0,"jobId":"29795065","backend":"gizmo","start":"2025-08-24T03:33:51.589Z","backendStatus":"Done","compressedDockerSize":"2871239351","end":"2025-08-24T03:34:45.863Z","dockerImageUsed":"tensorflow/tensorflow@sha256:67f1a7b35fd52bdda071c0cd311655be7477f2bc1b6f27e014b9a57231bd55b3","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest","attempt":1,"executionEvents":[{"startTime":"2025-08-24T03:34:45.213Z","description":"UpdatingJobStore","endTime":"2025-08-24T03:34:45.863Z"},{"startTime":"2025-08-24T03:34:00.116Z","description":"PreparingJob","endTime":"2025-08-24T03:34:00.654Z"},{"startTime":"2025-08-24T03:34:00.654Z","description":"RunningJob","endTime":"2025-08-24T03:34:45.213Z"},{"startTime":"2025-08-24T03:33:51.590Z","description":"RequestingExecutionToken","endTime":"2025-08-24T03:34:00.116Z"},{"startTime":"2025-08-24T03:34:00.116Z","description":"WaitingForValueStore","endTime":"2025-08-24T03:34:00.116Z"},{"startTime":"2025-08-24T03:33:51.589Z","description":"Pending","endTime":"2025-08-24T03:33:51.590Z"}]}]},"outputs":{"GpuMatrixMult.test_results":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75/call-GpuTest/execution/stdout","GpuMatrixMult.matrix_result":[22.0,28.0,49.0,64.0],"GpuMatrixMult.gpu_count":1},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a83bf8ab-3ff4-4695-b174-6520a23b3d75","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-08-24T03:34:47.690Z","start":"2025-08-24T03:33:50.519Z","id":"a83bf8ab-3ff4-4695-b174-6520a23b3d75","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a83bf8ab-3ff4-4695-b174-6520a23b3d75"},"submission":"2025-08-24T03:33:10.361Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 24 Aug 2025 03:34:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '7182'
    status:
      code: 200
      message: OK
version: 1
