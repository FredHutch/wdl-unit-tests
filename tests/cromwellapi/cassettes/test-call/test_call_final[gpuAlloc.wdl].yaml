interactions:
- request:
    body: "--0a5d545d933cf9e6eaa327d8ad424666\r\nContent-Disposition: form-data; name=\"workflowSource\";
      filename=\"gpuAlloc.wdl\"\r\nContent-Type: application/octet-stream\r\n\r\nversion
      1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
      TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
      2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication works
      correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File
      test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n
      \       Array[Float] matrix_result = GpuTest.multiplication_result\n    }\n\n
      \   parameter_meta {\n        test_results: \"Complete log of the GPU test including
      tensor operations and GPU detection\"\n        gpu_count: \"Number of GPUs detected
      by TensorFlow\"\n        matrix_result: \"Results of the matrix multiplication
      operation\"\n    }\n}\n\ntask GpuTest {\n    command <<<\n        python3 <<CODE\n
      \       import tensorflow as tf\n        import numpy as np\n\n        # Test
      GPU availability\n        gpus = tf.config.experimental.list_physical_devices('GPU')\n
      \       detected_gpus = len(gpus)\n        print(f\"Number of GPUs detected:
      {detected_gpus}\")\n\n        # Verify GPU allocation matches runtime specification\n
      \       expected_gpus = 1  # Matches the runtime.gpus specification\n        if
      detected_gpus != expected_gpus:\n            raise RuntimeError(f\"GPU allocation
      mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n
      \       # Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0,
      4.0, 5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0,
      4.0, 5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n
      \       result = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n
      \       print(\"\\nMatrix A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix
      B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix Multiplication
      Result:\")\n        print(result.numpy())\n        \n        # Save results
      for output\n        np.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\n
      \       with open(\"gpu_count.txt\", \"w\") as f:\n            f.write(str(len(gpus)))\n
      \       CODE\n    >>>\n\n    output {\n        File results = stdout()\n        Int
      detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float] multiplication_result
      = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime {\n        docker:
      \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules: \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n
      \       gpus: \"1\"\n    }\n\n    parameter_meta {\n        results: \"Output
      log containing GPU detection and matrix multiplication results\"\n        detected_gpus:
      \"Number of GPUs detected by TensorFlow\"\n        multiplication_result: \"Flattened
      array containing the result of matrix multiplication\"\n    }\n}\n\r\n--0a5d545d933cf9e6eaa327d8ad424666\r\nContent-Disposition:
      form-data; name=\"workflowOptions\"; filename=\"options.json\"\r\nContent-Type:
      application/json\r\n\r\n{\n    \"workflow_failure_mode\": \"ContinueWhilePossible\",\n
      \   \"write_to_cache\": false,\n    \"read_from_cache\": false\n}\n\r\n--0a5d545d933cf9e6eaa327d8ad424666--\r\n"
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '3160'
      Content-Type:
      - multipart/form-data; boundary=0a5d545d933cf9e6eaa327d8ad424666
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1
  response:
    body:
      string: '{"id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","status":"Submitted"}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '66'
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:49:32 GMT
      Server:
      - nginx/1.25.3
    status:
      code: 201
      message: Created
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:49:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:49:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:49:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:49:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:49:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:50:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:51:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:52:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:53:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:54:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:55:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:55:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:55:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:55:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:55:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:55:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:55:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:55:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:55:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:55:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:55:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:56:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:57:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:58:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 03:59:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:00:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:01:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:02:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:03:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:04:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:05:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:05:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:05:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:05:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:05:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:05:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:05:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:05:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:05:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:05:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:05:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:06:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:07:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:08:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-3d319d1","description":"PickedUp","timestamp":"2026-01-25T04:09:49.156Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"QueuedInCromwell","shardIndex":-1,"backend":"gizmo","attempt":1,"start":"2026-01-25T04:09:50.201Z"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9","actualWorkflowLanguage":"WDL","status":"Running","start":"2026-01-25T04:09:49.157Z","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3807'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-3d319d1","description":"PickedUp","timestamp":"2026-01-25T04:09:49.156Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"QueuedInCromwell","shardIndex":-1,"backend":"gizmo","attempt":1,"start":"2026-01-25T04:09:50.201Z"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9","actualWorkflowLanguage":"WDL","status":"Running","start":"2026-01-25T04:09:49.157Z","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:09:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3807'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-3d319d1","description":"PickedUp","timestamp":"2026-01-25T04:09:49.156Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stdout","compressedDockerSize":"2871239351","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"backend":"gizmo","attempt":1,"start":"2026-01-25T04:09:50.201Z","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9","actualWorkflowLanguage":"WDL","status":"Running","start":"2026-01-25T04:09:49.157Z","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:10:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5682'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-3d319d1","description":"PickedUp","timestamp":"2026-01-25T04:09:49.156Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"45591550","backend":"gizmo","attempt":1,"start":"2026-01-25T04:09:50.201Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9","actualWorkflowLanguage":"WDL","status":"Running","start":"2026-01-25T04:09:49.157Z","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:10:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-3d319d1","description":"PickedUp","timestamp":"2026-01-25T04:09:49.156Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"45591550","backend":"gizmo","attempt":1,"start":"2026-01-25T04:09:50.201Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9","actualWorkflowLanguage":"WDL","status":"Running","start":"2026-01-25T04:09:49.157Z","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:10:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-3d319d1","description":"PickedUp","timestamp":"2026-01-25T04:09:49.156Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"45591550","backend":"gizmo","attempt":1,"start":"2026-01-25T04:09:50.201Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9","actualWorkflowLanguage":"WDL","status":"Running","start":"2026-01-25T04:09:49.157Z","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:10:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-3d319d1","description":"PickedUp","timestamp":"2026-01-25T04:09:49.156Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"45591550","backend":"gizmo","attempt":1,"start":"2026-01-25T04:09:50.201Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9","actualWorkflowLanguage":"WDL","status":"Running","start":"2026-01-25T04:09:49.157Z","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:10:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-3d319d1","description":"PickedUp","timestamp":"2026-01-25T04:09:49.156Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"45591550","backend":"gizmo","attempt":1,"start":"2026-01-25T04:09:50.201Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9","actualWorkflowLanguage":"WDL","status":"Running","start":"2026-01-25T04:09:49.157Z","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:10:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-3d319d1","description":"PickedUp","timestamp":"2026-01-25T04:09:49.156Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"45591550","backend":"gizmo","attempt":1,"start":"2026-01-25T04:09:50.201Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9","actualWorkflowLanguage":"WDL","status":"Running","start":"2026-01-25T04:09:49.157Z","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:10:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-3d319d1","description":"PickedUp","timestamp":"2026-01-25T04:09:49.156Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"45591550","backend":"gizmo","dockerImageUsed":"tensorflow/tensorflow@sha256:67f1a7b35fd52bdda071c0cd311655be7477f2bc1b6f27e014b9a57231bd55b3","attempt":1,"start":"2026-01-25T04:09:50.201Z","backendStatus":"Done","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9","actualWorkflowLanguage":"WDL","status":"Running","start":"2026-01-25T04:09:49.157Z","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:10:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5838'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok38.fhcrc.org:44395
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok38.fhcrc.org:44395/api/workflows/v1/a52ea711-2f5d-4b1b-b48f-261c75b278c9/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-3d319d1","description":"Finished","timestamp":"2026-01-25T04:10:38.138Z","cromwellVersion":"87"},{"cromwellId":"cromid-3d319d1","description":"PickedUp","timestamp":"2026-01-25T04:09:49.156Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Done","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"outputs":{"multiplication_result":[22.0,28.0,49.0,64.0],"results":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stdout","detected_gpus":1},"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"returnCode":0,"jobId":"45591550","backend":"gizmo","start":"2026-01-25T04:09:50.201Z","backendStatus":"Done","compressedDockerSize":"2871239351","end":"2026-01-25T04:10:36.630Z","dockerImageUsed":"tensorflow/tensorflow@sha256:67f1a7b35fd52bdda071c0cd311655be7477f2bc1b6f27e014b9a57231bd55b3","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest","attempt":1,"executionEvents":[{"startTime":"2026-01-25T04:09:56.950Z","description":"RunningJob","endTime":"2026-01-25T04:10:35.750Z"},{"startTime":"2026-01-25T04:09:56.935Z","description":"WaitingForValueStore","endTime":"2026-01-25T04:09:56.936Z"},{"startTime":"2026-01-25T04:09:56.936Z","description":"PreparingJob","endTime":"2026-01-25T04:09:56.950Z"},{"startTime":"2026-01-25T04:10:35.750Z","description":"UpdatingJobStore","endTime":"2026-01-25T04:10:36.630Z"},{"startTime":"2026-01-25T04:09:50.202Z","description":"RequestingExecutionToken","endTime":"2026-01-25T04:09:56.935Z"},{"startTime":"2026-01-25T04:09:50.201Z","description":"Pending","endTime":"2026-01-25T04:09:50.202Z"}]}]},"outputs":{"GpuMatrixMult.gpu_count":1,"GpuMatrixMult.test_results":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9/call-GpuTest/execution/stdout","GpuMatrixMult.matrix_result":[22.0,28.0,49.0,64.0]},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/a52ea711-2f5d-4b1b-b48f-261c75b278c9","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2026-01-25T04:10:38.138Z","start":"2026-01-25T04:09:49.157Z","id":"a52ea711-2f5d-4b1b-b48f-261c75b278c9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a52ea711-2f5d-4b1b-b48f-261c75b278c9"},"submission":"2026-01-25T03:49:32.457Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 25 Jan 2026 04:10:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '7182'
    status:
      code: 200
      message: OK
version: 1
