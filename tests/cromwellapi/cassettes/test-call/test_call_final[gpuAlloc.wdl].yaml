interactions:
- request:
    body: "--1c5ccceeff694677ab7f546a47aba5c3\r\nContent-Disposition: form-data; name=\"workflowSource\";
      filename=\"gpuAlloc.wdl\"\r\nContent-Type: application/octet-stream\r\n\r\nversion
      1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
      TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
      2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication works
      correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File
      test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n
      \       Array[Float] matrix_result = GpuTest.multiplication_result\n    }\n\n
      \   parameter_meta {\n        test_results: \"Complete log of the GPU test including
      tensor operations and GPU detection\"\n        gpu_count: \"Number of GPUs detected
      by TensorFlow\"\n        matrix_result: \"Results of the matrix multiplication
      operation\"\n    }\n}\n\ntask GpuTest {\n    command <<<\n        python3 <<CODE\n
      \       import tensorflow as tf\n        import numpy as np\n\n        # Test
      GPU availability\n        gpus = tf.config.experimental.list_physical_devices('GPU')\n
      \       detected_gpus = len(gpus)\n        print(f\"Number of GPUs detected:
      {detected_gpus}\")\n\n        # Verify GPU allocation matches runtime specification\n
      \       expected_gpus = 1  # Matches the runtime.gpus specification\n        if
      detected_gpus != expected_gpus:\n            raise RuntimeError(f\"GPU allocation
      mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n
      \       # Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0,
      4.0, 5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0,
      4.0, 5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n
      \       result = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n
      \       print(\"\\nMatrix A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix
      B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix Multiplication
      Result:\")\n        print(result.numpy())\n        \n        # Save results
      for output\n        np.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\n
      \       with open(\"gpu_count.txt\", \"w\") as f:\n            f.write(str(len(gpus)))\n
      \       CODE\n    >>>\n\n    output {\n        File results = stdout()\n        Int
      detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float] multiplication_result
      = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime {\n        docker:
      \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules: \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n
      \       gpus: \"1\"\n    }\n\n    parameter_meta {\n        results: \"Output
      log containing GPU detection and matrix multiplication results\"\n        detected_gpus:
      \"Number of GPUs detected by TensorFlow\"\n        multiplication_result: \"Flattened
      array containing the result of matrix multiplication\"\n    }\n}\n\r\n--1c5ccceeff694677ab7f546a47aba5c3\r\nContent-Disposition:
      form-data; name=\"workflowOptions\"; filename=\"options.json\"\r\nContent-Type:
      application/json\r\n\r\n{\n    \"workflow_failure_mode\": \"ContinueWhilePossible\",\n
      \   \"write_to_cache\": false,\n    \"read_from_cache\": false\n}\n\r\n--1c5ccceeff694677ab7f546a47aba5c3--\r\n"
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '3160'
      content-type:
      - multipart/form-data; boundary=1c5ccceeff694677ab7f546a47aba5c3
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1
  response:
    body:
      string: '{"id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","status":"Submitted"}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '66'
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:46:25 GMT
      Server:
      - nginx/1.25.3
    status:
      code: 201
      message: Created
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:46:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:46:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:46:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:46:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:46:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:46:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:47:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:48:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:49:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:50:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:51:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:52:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:53:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:54:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:55:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:55:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:55:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:55:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:55:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:55:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:55:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:55:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:55:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:55:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:55:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:56:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:57:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:58:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 03:59:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:00:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:01:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:02:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:03:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:04:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:05:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:05:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:05:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:05:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:05:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:05:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:05:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:05:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:05:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:05:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:05:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:06:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-df37326","description":"PickedUp","timestamp":"2025-11-30T04:07:33.145Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"QueuedInCromwell","shardIndex":-1,"backend":"gizmo","attempt":1,"start":"2025-11-30T04:07:34.186Z"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-30T04:07:33.145Z","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3807'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-df37326","description":"PickedUp","timestamp":"2025-11-30T04:07:33.145Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"QueuedInCromwell","shardIndex":-1,"backend":"gizmo","attempt":1,"start":"2025-11-30T04:07:34.186Z"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-30T04:07:33.145Z","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3807'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-df37326","description":"PickedUp","timestamp":"2025-11-30T04:07:33.145Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","compressedDockerSize":"2871239351","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"backend":"gizmo","attempt":1,"start":"2025-11-30T04:07:34.186Z","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-30T04:07:33.145Z","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5682'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-df37326","description":"PickedUp","timestamp":"2025-11-30T04:07:33.145Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","compressedDockerSize":"2871239351","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"backend":"gizmo","attempt":1,"start":"2025-11-30T04:07:34.186Z","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-30T04:07:33.145Z","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5682'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-df37326","description":"PickedUp","timestamp":"2025-11-30T04:07:33.145Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"43009711","backend":"gizmo","attempt":1,"start":"2025-11-30T04:07:34.186Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-30T04:07:33.145Z","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:07:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-df37326","description":"PickedUp","timestamp":"2025-11-30T04:07:33.145Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"43009711","backend":"gizmo","attempt":1,"start":"2025-11-30T04:07:34.186Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-30T04:07:33.145Z","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:08:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-df37326","description":"PickedUp","timestamp":"2025-11-30T04:07:33.145Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"43009711","backend":"gizmo","attempt":1,"start":"2025-11-30T04:07:34.186Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-30T04:07:33.145Z","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:08:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-df37326","description":"PickedUp","timestamp":"2025-11-30T04:07:33.145Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"43009711","backend":"gizmo","attempt":1,"start":"2025-11-30T04:07:34.186Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-30T04:07:33.145Z","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:08:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-df37326","description":"PickedUp","timestamp":"2025-11-30T04:07:33.145Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"43009711","backend":"gizmo","attempt":1,"start":"2025-11-30T04:07:34.186Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-30T04:07:33.145Z","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:08:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-df37326","description":"PickedUp","timestamp":"2025-11-30T04:07:33.145Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Done","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"outputs":{"multiplication_result":[22.0,28.0,49.0,64.0],"results":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","detected_gpus":1},"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"returnCode":0,"jobId":"43009711","backend":"gizmo","start":"2025-11-30T04:07:34.186Z","backendStatus":"Done","compressedDockerSize":"2871239351","end":"2025-11-30T04:08:19.899Z","dockerImageUsed":"tensorflow/tensorflow@sha256:67f1a7b35fd52bdda071c0cd311655be7477f2bc1b6f27e014b9a57231bd55b3","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest","attempt":1,"executionEvents":[{"startTime":"2025-11-30T04:07:34.186Z","description":"RequestingExecutionToken","endTime":"2025-11-30T04:07:41.144Z"},{"startTime":"2025-11-30T04:07:34.186Z","description":"Pending","endTime":"2025-11-30T04:07:34.186Z"},{"startTime":"2025-11-30T04:07:41.144Z","description":"WaitingForValueStore","endTime":"2025-11-30T04:07:41.144Z"},{"startTime":"2025-11-30T04:07:41.156Z","description":"RunningJob","endTime":"2025-11-30T04:08:19.814Z"},{"startTime":"2025-11-30T04:08:19.814Z","description":"UpdatingJobStore","endTime":"2025-11-30T04:08:19.899Z"},{"startTime":"2025-11-30T04:07:41.144Z","description":"PreparingJob","endTime":"2025-11-30T04:07:41.156Z"}]}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-30T04:07:33.145Z","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:08:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '6784'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj12.fhcrc.org:34331
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj12.fhcrc.org:34331/api/workflows/v1/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-df37326","description":"PickedUp","timestamp":"2025-11-30T04:07:33.145Z","cromwellVersion":"87"},{"cromwellId":"cromid-df37326","description":"Finished","timestamp":"2025-11-30T04:08:21.107Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Done","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"outputs":{"multiplication_result":[22.0,28.0,49.0,64.0],"results":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","detected_gpus":1},"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"returnCode":0,"jobId":"43009711","backend":"gizmo","start":"2025-11-30T04:07:34.186Z","backendStatus":"Done","compressedDockerSize":"2871239351","end":"2025-11-30T04:08:19.899Z","dockerImageUsed":"tensorflow/tensorflow@sha256:67f1a7b35fd52bdda071c0cd311655be7477f2bc1b6f27e014b9a57231bd55b3","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest","attempt":1,"executionEvents":[{"startTime":"2025-11-30T04:07:34.186Z","description":"RequestingExecutionToken","endTime":"2025-11-30T04:07:41.144Z"},{"startTime":"2025-11-30T04:07:34.186Z","description":"Pending","endTime":"2025-11-30T04:07:34.186Z"},{"startTime":"2025-11-30T04:07:41.144Z","description":"WaitingForValueStore","endTime":"2025-11-30T04:07:41.144Z"},{"startTime":"2025-11-30T04:07:41.156Z","description":"RunningJob","endTime":"2025-11-30T04:08:19.814Z"},{"startTime":"2025-11-30T04:08:19.814Z","description":"UpdatingJobStore","endTime":"2025-11-30T04:08:19.899Z"},{"startTime":"2025-11-30T04:07:41.144Z","description":"PreparingJob","endTime":"2025-11-30T04:07:41.156Z"}]}]},"outputs":{"GpuMatrixMult.gpu_count":1,"GpuMatrixMult.test_results":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303/call-GpuTest/execution/stdout","GpuMatrixMult.matrix_result":[22.0,28.0,49.0,64.0]},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/f5192d5c-b30d-4f02-bfdf-ee1825ed5303","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-11-30T04:08:21.107Z","start":"2025-11-30T04:07:33.145Z","id":"f5192d5c-b30d-4f02-bfdf-ee1825ed5303","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-f5192d5c-b30d-4f02-bfdf-ee1825ed5303"},"submission":"2025-11-30T03:46:25.933Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 30 Nov 2025 04:08:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '7182'
    status:
      code: 200
      message: OK
version: 1
