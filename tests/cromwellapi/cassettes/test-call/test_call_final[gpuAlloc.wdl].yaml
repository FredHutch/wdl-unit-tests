interactions:
- request:
    body: "--dc07ae4f40581464caed71bc79a3b701\r\nContent-Disposition: form-data; name=\"workflowSource\";
      filename=\"gpuAlloc.wdl\"\r\nContent-Type: application/octet-stream\r\n\r\nversion
      1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
      TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
      2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication works
      correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File
      test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n
      \       Array[Float] matrix_result = GpuTest.multiplication_result\n    }\n\n
      \   parameter_meta {\n        test_results: \"Complete log of the GPU test including
      tensor operations and GPU detection\"\n        gpu_count: \"Number of GPUs detected
      by TensorFlow\"\n        matrix_result: \"Results of the matrix multiplication
      operation\"\n    }\n}\n\ntask GpuTest {\n    command <<<\n        python3 <<CODE\n
      \       import tensorflow as tf\n        import numpy as np\n\n        # Test
      GPU availability\n        gpus = tf.config.experimental.list_physical_devices('GPU')\n
      \       detected_gpus = len(gpus)\n        print(f\"Number of GPUs detected:
      {detected_gpus}\")\n\n        # Verify GPU allocation matches runtime specification\n
      \       expected_gpus = 1  # Matches the runtime.gpus specification\n        if
      detected_gpus != expected_gpus:\n            raise RuntimeError(f\"GPU allocation
      mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n
      \       # Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0,
      4.0, 5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0,
      4.0, 5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n
      \       result = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n
      \       print(\"\\nMatrix A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix
      B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix Multiplication
      Result:\")\n        print(result.numpy())\n        \n        # Save results
      for output\n        np.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\n
      \       with open(\"gpu_count.txt\", \"w\") as f:\n            f.write(str(len(gpus)))\n
      \       CODE\n    >>>\n\n    output {\n        File results = stdout()\n        Int
      detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float] multiplication_result
      = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime {\n        docker:
      \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules: \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n
      \       gpus: \"1\"\n    }\n\n    parameter_meta {\n        results: \"Output
      log containing GPU detection and matrix multiplication results\"\n        detected_gpus:
      \"Number of GPUs detected by TensorFlow\"\n        multiplication_result: \"Flattened
      array containing the result of matrix multiplication\"\n    }\n}\n\r\n--dc07ae4f40581464caed71bc79a3b701\r\nContent-Disposition:
      form-data; name=\"workflowOptions\"; filename=\"options.json\"\r\nContent-Type:
      application/json\r\n\r\n{\n    \"workflow_failure_mode\": \"ContinueWhilePossible\",\n
      \   \"write_to_cache\": false,\n    \"read_from_cache\": false\n}\n\r\n--dc07ae4f40581464caed71bc79a3b701--\r\n"
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '3160'
      content-type:
      - multipart/form-data; boundary=dc07ae4f40581464caed71bc79a3b701
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1
  response:
    body:
      string: '{"id":"58fcbe1f-91fb-467e-8214-3921d463a014","status":"Submitted"}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '66'
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:42:38 GMT
      Server:
      - nginx/1.25.3
    status:
      code: 201
      message: Created
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:42:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:42:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:42:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:42:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:43:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:44:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:44:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:44:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:44:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:44:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:44:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:44:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:44:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:44:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:44:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:44:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:45:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:46:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:47:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:48:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:49:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:50:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:51:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:52:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:53:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:54:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:55:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:55:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:55:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:55:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:55:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:55:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:55:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:55:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:55:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:55:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:55:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:56:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:57:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:58:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 03:59:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:00:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:01:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:02:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:03:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:04:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:05:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:05:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:05:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:05:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:05:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:05:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:05:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:05:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:05:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:05:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:05:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n\n# This workflow tests
        GPU allocation and basic tensor operations using TensorFlow\n# It verifies
        that:\n# 1. A GPU can be successfully allocated\n# 2. TensorFlow can detect
        and use the GPU\n# 3. Basic matrix multiplication works correctly\n\nworkflow
        GpuMatrixMult {\n    call GpuTest\n\n    output {\n        File test_results
        = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3260'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-7248ac8","description":"PickedUp","timestamp":"2025-11-02T04:06:15.670Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"QueuedInCromwell","shardIndex":-1,"backend":"gizmo","attempt":1,"start":"2025-11-02T04:06:16.714Z"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-02T04:06:15.671Z","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3807'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-7248ac8","description":"PickedUp","timestamp":"2025-11-02T04:06:15.670Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","compressedDockerSize":"2871239351","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"backend":"gizmo","attempt":1,"start":"2025-11-02T04:06:16.714Z","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-02T04:06:15.671Z","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5682'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-7248ac8","description":"PickedUp","timestamp":"2025-11-02T04:06:15.670Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"41177142","backend":"gizmo","attempt":1,"start":"2025-11-02T04:06:16.714Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-02T04:06:15.671Z","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-7248ac8","description":"PickedUp","timestamp":"2025-11-02T04:06:15.670Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"41177142","backend":"gizmo","attempt":1,"start":"2025-11-02T04:06:16.714Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-02T04:06:15.671Z","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-7248ac8","description":"PickedUp","timestamp":"2025-11-02T04:06:15.670Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"41177142","backend":"gizmo","attempt":1,"start":"2025-11-02T04:06:16.714Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-02T04:06:15.671Z","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-7248ac8","description":"PickedUp","timestamp":"2025-11-02T04:06:15.670Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"41177142","backend":"gizmo","attempt":1,"start":"2025-11-02T04:06:16.714Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-02T04:06:15.671Z","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-7248ac8","description":"PickedUp","timestamp":"2025-11-02T04:06:15.670Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"41177142","backend":"gizmo","attempt":1,"start":"2025-11-02T04:06:16.714Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-02T04:06:15.671Z","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-7248ac8","description":"PickedUp","timestamp":"2025-11-02T04:06:15.670Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"41177142","backend":"gizmo","attempt":1,"start":"2025-11-02T04:06:16.714Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-02T04:06:15.671Z","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:06:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-7248ac8","description":"PickedUp","timestamp":"2025-11-02T04:06:15.670Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Running","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"jobId":"41177142","backend":"gizmo","attempt":1,"start":"2025-11-02T04:06:16.714Z","backendStatus":"Running","compressedDockerSize":"2871239351","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest"}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-02T04:06:15.671Z","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:07:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '5727'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-7248ac8","description":"PickedUp","timestamp":"2025-11-02T04:06:15.670Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Done","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"outputs":{"multiplication_result":[22.0,28.0,49.0,64.0],"results":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","detected_gpus":1},"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"returnCode":0,"jobId":"41177142","backend":"gizmo","start":"2025-11-02T04:06:16.714Z","backendStatus":"Done","compressedDockerSize":"2871239351","end":"2025-11-02T04:07:02.777Z","dockerImageUsed":"tensorflow/tensorflow@sha256:67f1a7b35fd52bdda071c0cd311655be7477f2bc1b6f27e014b9a57231bd55b3","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest","attempt":1,"executionEvents":[{"startTime":"2025-11-02T04:06:19.911Z","description":"RunningJob","endTime":"2025-11-02T04:07:02.450Z"},{"startTime":"2025-11-02T04:07:02.450Z","description":"UpdatingJobStore","endTime":"2025-11-02T04:07:02.777Z"},{"startTime":"2025-11-02T04:06:19.183Z","description":"PreparingJob","endTime":"2025-11-02T04:06:19.911Z"},{"startTime":"2025-11-02T04:06:16.714Z","description":"Pending","endTime":"2025-11-02T04:06:16.714Z"},{"startTime":"2025-11-02T04:06:19.182Z","description":"WaitingForValueStore","endTime":"2025-11-02T04:06:19.183Z"},{"startTime":"2025-11-02T04:06:16.714Z","description":"RequestingExecutionToken","endTime":"2025-11-02T04:06:19.182Z"}]}]},"outputs":{},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014","actualWorkflowLanguage":"WDL","status":"Running","start":"2025-11-02T04:06:15.671Z","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:07:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '6784'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok49.fhcrc.org:32951
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok49.fhcrc.org:32951/api/workflows/v1/58fcbe1f-91fb-467e-8214-3921d463a014/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowName":"GpuMatrixMult","workflowProcessingEvents":[{"cromwellId":"cromid-7248ac8","description":"PickedUp","timestamp":"2025-11-02T04:06:15.670Z","cromwellVersion":"87"},{"cromwellId":"cromid-7248ac8","description":"Finished","timestamp":"2025-11-02T04:07:04.655Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# This workflow tests GPU allocation and basic tensor operations using
        TensorFlow\n# It verifies that:\n# 1. A GPU can be successfully allocated\n#
        2. TensorFlow can detect and use the GPU\n# 3. Basic matrix multiplication
        works correctly\n\nworkflow GpuMatrixMult {\n    call GpuTest\n\n    output
        {\n        File test_results = GpuTest.results\n        Int gpu_count = GpuTest.detected_gpus\n        Array[Float]
        matrix_result = GpuTest.multiplication_result\n    }\n\n    parameter_meta
        {\n        test_results: \"Complete log of the GPU test including tensor operations
        and GPU detection\"\n        gpu_count: \"Number of GPUs detected by TensorFlow\"\n        matrix_result:
        \"Results of the matrix multiplication operation\"\n    }\n}\n\ntask GpuTest
        {\n    command <<<\n        python3 <<CODE\n        import tensorflow as tf\n        import
        numpy as np\n\n        # Test GPU availability\n        gpus = tf.config.experimental.list_physical_devices(''GPU'')\n        detected_gpus
        = len(gpus)\n        print(f\"Number of GPUs detected: {detected_gpus}\")\n\n        #
        Verify GPU allocation matches runtime specification\n        expected_gpus
        = 1  # Matches the runtime.gpus specification\n        if detected_gpus !=
        expected_gpus:\n            raise RuntimeError(f\"GPU allocation mismatch:
        Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n        \n        #
        Create test matrices\n        matrix_a = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[2, 3])\n        matrix_b = tf.constant([1.0, 2.0, 3.0, 4.0,
        5.0, 6.0], shape=[3, 2])\n        \n        # Perform matrix multiplication\n        result
        = tf.matmul(matrix_a, matrix_b)\n        \n        # Print results\n        print(\"\\nMatrix
        A:\")\n        print(matrix_a.numpy())\n        print(\"\\nMatrix B:\")\n        print(matrix_b.numpy())\n        print(\"\\nMatrix
        Multiplication Result:\")\n        print(result.numpy())\n        \n        #
        Save results for output\n        np.savetxt(\"multiplication_result.txt\",
        result.numpy().flatten())\n        with open(\"gpu_count.txt\", \"w\") as
        f:\n            f.write(str(len(gpus)))\n        CODE\n    >>>\n\n    output
        {\n        File results = stdout()\n        Int detected_gpus = read_int(\"gpu_count.txt\")\n        Array[Float]
        multiplication_result = read_lines(\"multiplication_result.txt\")\n    }\n\n    runtime
        {\n        docker: \"tensorflow/tensorflow:2.11.0-gpu\"\n        # modules:
        \"TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0\"\n        gpus: \"1\"\n    }\n\n    parameter_meta
        {\n        results: \"Output log containing GPU detection and matrix multiplication
        results\"\n        detected_gpus: \"Number of GPUs detected by TensorFlow\"\n        multiplication_result:
        \"Flattened array containing the result of matrix multiplication\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{"GpuMatrixMult.GpuTest":[{"executionStatus":"Done","stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","commandLine":"python3
        <<CODE\nimport tensorflow as tf\nimport numpy as np\n\n# Test GPU availability\ngpus
        = tf.config.experimental.list_physical_devices(''GPU'')\ndetected_gpus = len(gpus)\nprint(f\"Number
        of GPUs detected: {detected_gpus}\")\n\n# Verify GPU allocation matches runtime
        specification\nexpected_gpus = 1  # Matches the runtime.gpus specification\nif
        detected_gpus != expected_gpus:\n    raise RuntimeError(f\"GPU allocation
        mismatch: Expected {expected_gpus} GPU(s), but found {detected_gpus}\")\n\n#
        Create test matrices\nmatrix_a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
        shape=[2, 3])\nmatrix_b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,
        2])\n\n# Perform matrix multiplication\nresult = tf.matmul(matrix_a, matrix_b)\n\n#
        Print results\nprint(\"\\nMatrix A:\")\nprint(matrix_a.numpy())\nprint(\"\\nMatrix
        B:\")\nprint(matrix_b.numpy())\nprint(\"\\nMatrix Multiplication Result:\")\nprint(result.numpy())\n\n#
        Save results for output\nnp.savetxt(\"multiplication_result.txt\", result.numpy().flatten())\nwith
        open(\"gpu_count.txt\", \"w\") as f:\n    f.write(str(len(gpus)))\nCODE","shardIndex":-1,"outputs":{"multiplication_result":[22.0,28.0,49.0,64.0],"results":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","detected_gpus":1},"runtimeAttributes":{"failOnStderr":"false","partition":"campus-new","continueOnReturnCode":"0","docker":"tensorflow/tensorflow:2.11.0-gpu","modules":"","gpus":"1","maxRetries":"0","cpu":"1","memory":"1.953125
        GB"},"callCaching":{"allowResultReuse":false,"effectiveCallCachingMode":"CallCachingOff"},"inputs":{},"returnCode":0,"jobId":"41177142","backend":"gizmo","start":"2025-11-02T04:06:16.714Z","backendStatus":"Done","compressedDockerSize":"2871239351","end":"2025-11-02T04:07:02.777Z","dockerImageUsed":"tensorflow/tensorflow@sha256:67f1a7b35fd52bdda071c0cd311655be7477f2bc1b6f27e014b9a57231bd55b3","stderr":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stderr","callRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest","attempt":1,"executionEvents":[{"startTime":"2025-11-02T04:06:19.911Z","description":"RunningJob","endTime":"2025-11-02T04:07:02.450Z"},{"startTime":"2025-11-02T04:07:02.450Z","description":"UpdatingJobStore","endTime":"2025-11-02T04:07:02.777Z"},{"startTime":"2025-11-02T04:06:19.183Z","description":"PreparingJob","endTime":"2025-11-02T04:06:19.911Z"},{"startTime":"2025-11-02T04:06:16.714Z","description":"Pending","endTime":"2025-11-02T04:06:16.714Z"},{"startTime":"2025-11-02T04:06:19.182Z","description":"WaitingForValueStore","endTime":"2025-11-02T04:06:19.183Z"},{"startTime":"2025-11-02T04:06:16.714Z","description":"RequestingExecutionToken","endTime":"2025-11-02T04:06:19.182Z"}]}]},"outputs":{"GpuMatrixMult.test_results":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014/call-GpuTest/execution/stdout","GpuMatrixMult.matrix_result":[22.0,28.0,49.0,64.0],"GpuMatrixMult.gpu_count":1},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/GpuMatrixMult/58fcbe1f-91fb-467e-8214-3921d463a014","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-11-02T04:07:04.655Z","start":"2025-11-02T04:06:15.671Z","id":"58fcbe1f-91fb-467e-8214-3921d463a014","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-58fcbe1f-91fb-467e-8214-3921d463a014"},"submission":"2025-11-02T03:42:38.975Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 02 Nov 2025 04:07:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '7182'
    status:
      code: 200
      message: OK
version: 1
