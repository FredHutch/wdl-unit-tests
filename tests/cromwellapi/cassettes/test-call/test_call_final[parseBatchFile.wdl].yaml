interactions:
- request:
    body: "--6cdaa3af2bc19a4927b32c4e090d5a5f\r\nContent-Disposition: form-data; name=\"workflowSource\";
      filename=\"parseBatchFile.wdl\"\r\nContent-Type: application/octet-stream\r\n\r\nversion
      1.0\n# This workflow takes a tab separated file where each row is a set of data
      to be used in each \n# of the independent scattered task series that you have
      as your workflow process.  This file \n# will, for example, have column names
      `sampleName`, `bamLocation`, and `bedlocation`.  This\n# allows you to know
      that regardless of the order of the columns in your batch file, the correct\n#
      inputs will be used for the tasks you define.  \nworkflow parseBatchFile {\n
      \ input {\n  File batchFile\n  }\n    Array[Object] batchInfo = read_objects(batchFile)\n
      \ scatter (job in batchInfo){\n    String sampleName = job.sampleName\n    File
      bamFile = job.bamLocation\n    File bedFile = job.bedLocation\n\n    ## INSERT
      YOUR WORKFLOW TO RUN PER LINE IN YOUR BATCH FILE HERE!!!!\n    call test {\n
      \       input: in1=sampleName, in2=bamFile, in3=bedFile\n    }\n\n  }  # End
      Scatter over the batch file\n# Outputs that will be retained when execution
      is complete\n  output {\n    Array[File] outputArray = test.item_out\n    }\n#
      End workflow\n}\n\n#### TASK DEFINITIONS\n# echo some text to stdout, treats
      files as strings just to echo them as a dummy example\ntask test {\n  input
      {\n    String in1\n    String in2\n    String in3\n  }\n    command {\n    echo
      ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output {\n        File
      item_out = stdout()\n    }\n}\r\n--6cdaa3af2bc19a4927b32c4e090d5a5f\r\nContent-Disposition:
      form-data; name=\"workflowOptions\"; filename=\"options.json\"\r\nContent-Type:
      application/json\r\n\r\n{\n    \"workflow_failure_mode\": \"ContinueWhilePossible\",\n
      \   \"write_to_cache\": false,\n    \"read_from_cache\": false\n}\n\r\n--6cdaa3af2bc19a4927b32c4e090d5a5f--\r\n"
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '1797'
      Content-Type:
      - multipart/form-data; boundary=6cdaa3af2bc19a4927b32c4e090d5a5f
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1
  response:
    body:
      string: '{"id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","status":"Submitted"}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '66'
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:44:55 GMT
      Server:
      - nginx/1.25.3
    status:
      code: 201
      message: Created
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:45:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:46:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:47:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:48:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:48:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:48:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:48:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:48:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:48:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:48:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:48:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:48:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:48:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:48:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:49:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:50:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:51:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:52:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:53:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:54:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:55:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:56:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:56:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:56:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:56:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:56:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:56:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:56:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:56:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:56:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:56:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:56:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:57:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:58:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 03:59:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:00:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:01:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:02:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:03:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:03:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:03:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:03:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:03:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:03:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:03:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:03:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:03:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:03:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:03:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:04:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:05:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:05:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:05:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:05:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:05:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Host:
      - gizmok92.fhcrc.org:41887
      User-Agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:41887/api/workflows/v1/35b448cb-a85d-49dd-93a6-e4075a0a6fe2/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowProcessingEvents":[{"cromwellId":"cromid-a3dbb85","description":"PickedUp","timestamp":"2025-12-21T04:05:22.793Z","cromwellVersion":"87"},{"cromwellId":"cromid-a3dbb85","description":"Finished","timestamp":"2025-12-21T04:05:22.836Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n# This workflow takes a tab separated file where each row is a set of
        data to be used in each \n# of the independent scattered task series that
        you have as your workflow process.  This file \n# will, for example, have
        column names `sampleName`, `bamLocation`, and `bedlocation`.  This\n# allows
        you to know that regardless of the order of the columns in your batch file,
        the correct\n# inputs will be used for the tasks you define.  \nworkflow parseBatchFile
        {\n  input {\n  File batchFile\n  }\n    Array[Object] batchInfo = read_objects(batchFile)\n  scatter
        (job in batchInfo){\n    String sampleName = job.sampleName\n    File bamFile
        = job.bamLocation\n    File bedFile = job.bedLocation\n\n    ## INSERT YOUR
        WORKFLOW TO RUN PER LINE IN YOUR BATCH FILE HERE!!!!\n    call test {\n        input:
        in1=sampleName, in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over
        the batch file\n# Outputs that will be retained when execution is complete\n  output
        {\n    Array[File] outputArray = test.item_out\n    }\n# End workflow\n}\n\n####
        TASK DEFINITIONS\n# echo some text to stdout, treats files as strings just
        to echo them as a dummy example\ntask test {\n  input {\n    String in1\n    String
        in2\n    String in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo
        ~{in3}\n    }\n    output {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Required
        workflow input ''parseBatchFile.batchFile'' not specified"}],"message":"Workflow
        input processing failed"}],"end":"2025-12-21T04:05:22.835Z","start":"2025-12-21T04:05:22.794Z","id":"35b448cb-a85d-49dd-93a6-e4075a0a6fe2","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b448cb-a85d-49dd-93a6-e4075a0a6fe2"},"submission":"2025-12-21T03:44:55.136Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Sun, 21 Dec 2025 04:05:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '2382'
    status:
      code: 200
      message: OK
version: 1
