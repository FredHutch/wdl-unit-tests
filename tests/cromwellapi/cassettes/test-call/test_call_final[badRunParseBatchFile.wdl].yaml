interactions:
- request:
    body: "--3ae92ab38c935da4f79afec68e00f20d\r\nContent-Disposition: form-data; name=\"workflowSource\";
      filename=\"badRunParseBatchFile.wdl\"\r\nContent-Type: application/octet-stream\r\n\r\nversion
      1.0\n# This workflow takes a tab separated file where each row is a set of data
      to be used in each \n# of the independent scattered task series that you have
      as your workflow process.  This file \n# will, for example, have column names
      `sampleName`, `bamLocation`, and `bedlocation`.  This\n# allows you to know
      that regardless of the order of the columns in your batch file, the correct\n#
      inputs will be used for the tasks you define.  \nworkflow parseBatchFile {\n
      \ input {\n  File batchFile\n  }\n    Array[Object] batchInfo = read_objects(batchFile)\n
      \ scatter (job in batchInfo){\n    String sampleName = job.sampleName\n    File
      bamFile = job.bamLocation\n    File bedFile = job.bedLocation\n\n    ## INSERT
      YOUR WORKFLOW TO RUN PER LINE IN YOUR BATCH FILE HERE!!!!\n    call test {\n
      \       input: in1=sampleName, in2=bamFile, in3=bedFile\n    }\n\n  }  # End
      Scatter over the batch file\n# Outputs that will be retained when execution
      is complete\n  output {\n    Array[File] outputArray = test.item_out\n    }\n#
      End workflow\n}\n\n#### TASK DEFINITIONS\n# echo some text to stdout, treats
      files as strings just to echo them as a dummy example\ntask test {\n  input
      {\n    String in1\n    String in2\n    String in3\n  }\n    command {\n    echo
      ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output {\n        File
      item_out = stdout()\n    }\n}\r\n--3ae92ab38c935da4f79afec68e00f20d\r\nContent-Disposition:
      form-data; name=\"workflowOptions\"; filename=\"options.json\"\r\nContent-Type:
      application/json\r\n\r\n{\n    \"workflow_failure_mode\": \"ContinueWhilePossible\",\n
      \   \"write_to_cache\": false,\n    \"read_from_cache\": false\n}\n\r\n--3ae92ab38c935da4f79afec68e00f20d--\r\n"
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '1803'
      content-type:
      - multipart/form-data; boundary=3ae92ab38c935da4f79afec68e00f20d
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1
  response:
    body:
      string: '{"id":"20774b7d-3e46-421d-9eb3-ef341f627c38","status":"Submitted"}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '66'
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:02:05 GMT
      Server:
      - nginx/1.25.3
    status:
      code: 201
      message: Created
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:02:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:02:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:02:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:02:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:02:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:02:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:02:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:02:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:02:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:02:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:03:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:04:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:05:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:06:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:07:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:07:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:07:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:07:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:07:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:07:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:07:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:07:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:07:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:07:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:07:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmoj4.fhcrc.org:35141
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmoj4.fhcrc.org:35141/api/workflows/v1/20774b7d-3e46-421d-9eb3-ef341f627c38/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowProcessingEvents":[{"cromwellId":"cromid-7df26a6","description":"Finished","timestamp":"2025-05-06T15:07:55.728Z","cromwellVersion":"87"},{"cromwellId":"cromid-7df26a6","description":"PickedUp","timestamp":"2025-05-06T15:07:55.715Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n# This workflow takes a tab separated file where each row is a set of
        data to be used in each \n# of the independent scattered task series that
        you have as your workflow process.  This file \n# will, for example, have
        column names `sampleName`, `bamLocation`, and `bedlocation`.  This\n# allows
        you to know that regardless of the order of the columns in your batch file,
        the correct\n# inputs will be used for the tasks you define.  \nworkflow parseBatchFile
        {\n  input {\n  File batchFile\n  }\n    Array[Object] batchInfo = read_objects(batchFile)\n  scatter
        (job in batchInfo){\n    String sampleName = job.sampleName\n    File bamFile
        = job.bamLocation\n    File bedFile = job.bedLocation\n\n    ## INSERT YOUR
        WORKFLOW TO RUN PER LINE IN YOUR BATCH FILE HERE!!!!\n    call test {\n        input:
        in1=sampleName, in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over
        the batch file\n# Outputs that will be retained when execution is complete\n  output
        {\n    Array[File] outputArray = test.item_out\n    }\n# End workflow\n}\n\n####
        TASK DEFINITIONS\n# echo some text to stdout, treats files as strings just
        to echo them as a dummy example\ntask test {\n  input {\n    String in1\n    String
        in2\n    String in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo
        ~{in3}\n    }\n    output {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Required
        workflow input ''parseBatchFile.batchFile'' not specified"}],"message":"Workflow
        input processing failed"}],"end":"2025-05-06T15:07:55.728Z","start":"2025-05-06T15:07:55.715Z","id":"20774b7d-3e46-421d-9eb3-ef341f627c38","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-20774b7d-3e46-421d-9eb3-ef341f627c38"},"submission":"2025-05-06T15:02:05.569Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 06 May 2025 15:08:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '2382'
    status:
      code: 200
      message: OK
version: 1
