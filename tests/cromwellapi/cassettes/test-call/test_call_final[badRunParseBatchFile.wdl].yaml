interactions:
- request:
    body: "--afd3e3b8a188530de0008781e9c139e6\r\nContent-Disposition: form-data; name=\"workflowSource\";
      filename=\"badRunParseBatchFile.wdl\"\r\nContent-Type: application/octet-stream\r\n\r\nversion
      1.0\n# This workflow takes a tab separated file where each row is a set of data
      to be used in each \n# of the independent scattered task series that you have
      as your workflow process.  This file \n# will, for example, have column names
      `sampleName`, `bamLocation`, and `bedlocation`.  This\n# allows you to know
      that regardless of the order of the columns in your batch file, the correct\n#
      inputs will be used for the tasks you define.  \nworkflow parseBatchFile {\n
      \ input {\n  File batchFile\n  }\n    Array[Object] batchInfo = read_objects(batchFile)\n
      \ scatter (job in batchInfo){\n    String sampleName = job.sampleName\n    File
      bamFile = job.bamLocation\n    File bedFile = job.bedLocation\n\n    ## INSERT
      YOUR WORKFLOW TO RUN PER LINE IN YOUR BATCH FILE HERE!!!!\n    call test {\n
      \       input: in1=sampleName, in2=bamFile, in3=bedFile\n    }\n\n  }  # End
      Scatter over the batch file\n# Outputs that will be retained when execution
      is complete\n  output {\n    Array[File] outputArray = test.item_out\n    }\n#
      End workflow\n}\n\n#### TASK DEFINITIONS\n# echo some text to stdout, treats
      files as strings just to echo them as a dummy example\ntask test {\n  input
      {\n    String in1\n    String in2\n    String in3\n  }\n    command {\n    echo
      ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output {\n        File
      item_out = stdout()\n    }\n}\r\n--afd3e3b8a188530de0008781e9c139e6\r\nContent-Disposition:
      form-data; name=\"workflowOptions\"; filename=\"options.json\"\r\nContent-Type:
      application/json\r\n\r\n{\n    \"workflow_failure_mode\": \"ContinueWhilePossible\",\n
      \   \"write_to_cache\": false,\n    \"read_from_cache\": false\n}\n\r\n--afd3e3b8a188530de0008781e9c139e6--\r\n"
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '1803'
      content-type:
      - multipart/form-data; boundary=afd3e3b8a188530de0008781e9c139e6
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1
  response:
    body:
      string: '{"id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","status":"Submitted"}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '66'
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:26:10 GMT
      Server:
      - nginx/1.25.3
    status:
      code: 201
      message: Created
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:26:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:26:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:26:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:26:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:26:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:26:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:26:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:26:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:26:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:27:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:28:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:29:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:30:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:31:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:31:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:31:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:31:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:31:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:31:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:31:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:31:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:31:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:31:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:31:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:32:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:26 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:31 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:36 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:41 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:46 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:51 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:33:56 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:01 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:16 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:21 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:32 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:37 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:42 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:47 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:34:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:12 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:17 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:22 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:27 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:35:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:54 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:36:59 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:37:04 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:37:09 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:37:14 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:37:19 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:37:24 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:37:29 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:37:34 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:37:39 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:37:44 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:37:49 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:37:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:05 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:10 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:15 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:20 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:25 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:30 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:35 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:40 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:45 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:50 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:38:55 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:39:00 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"submittedFiles":{"workflow":"version 1.0\n# This workflow takes a
        tab separated file where each row is a set of data to be used in each \n#
        of the independent scattered task series that you have as your workflow process.  This
        file \n# will, for example, have column names `sampleName`, `bamLocation`,
        and `bedlocation`.  This\n# allows you to know that regardless of the order
        of the columns in your batch file, the correct\n# inputs will be used for
        the tasks you define.  \nworkflow parseBatchFile {\n  input {\n  File batchFile\n  }\n    Array[Object]
        batchInfo = read_objects(batchFile)\n  scatter (job in batchInfo){\n    String
        sampleName = job.sampleName\n    File bamFile = job.bamLocation\n    File
        bedFile = job.bedLocation\n\n    ## INSERT YOUR WORKFLOW TO RUN PER LINE IN
        YOUR BATCH FILE HERE!!!!\n    call test {\n        input: in1=sampleName,
        in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over the batch file\n#
        Outputs that will be retained when execution is complete\n  output {\n    Array[File]
        outputArray = test.item_out\n    }\n# End workflow\n}\n\n#### TASK DEFINITIONS\n#
        echo some text to stdout, treats files as strings just to echo them as a dummy
        example\ntask test {\n  input {\n    String in1\n    String in2\n    String
        in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo ~{in3}\n    }\n    output
        {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"status":"Submitted","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:39:06 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1815'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok37.fhcrc.org:44649
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok37.fhcrc.org:44649/api/workflows/v1/caac32bb-a81a-46a3-8b9c-216c04a3743e/metadata?expandSubWorkflows=true
  response:
    body:
      string: '{"workflowProcessingEvents":[{"description":"Finished","cromwellVersion":"87","cromwellId":"cromid-32c4e56","timestamp":"2025-04-09T17:39:03.135Z"},{"cromwellId":"cromid-32c4e56","description":"PickedUp","timestamp":"2025-04-09T17:39:03.119Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n# This workflow takes a tab separated file where each row is a set of
        data to be used in each \n# of the independent scattered task series that
        you have as your workflow process.  This file \n# will, for example, have
        column names `sampleName`, `bamLocation`, and `bedlocation`.  This\n# allows
        you to know that regardless of the order of the columns in your batch file,
        the correct\n# inputs will be used for the tasks you define.  \nworkflow parseBatchFile
        {\n  input {\n  File batchFile\n  }\n    Array[Object] batchInfo = read_objects(batchFile)\n  scatter
        (job in batchInfo){\n    String sampleName = job.sampleName\n    File bamFile
        = job.bamLocation\n    File bedFile = job.bedLocation\n\n    ## INSERT YOUR
        WORKFLOW TO RUN PER LINE IN YOUR BATCH FILE HERE!!!!\n    call test {\n        input:
        in1=sampleName, in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over
        the batch file\n# Outputs that will be retained when execution is complete\n  output
        {\n    Array[File] outputArray = test.item_out\n    }\n# End workflow\n}\n\n####
        TASK DEFINITIONS\n# echo some text to stdout, treats files as strings just
        to echo them as a dummy example\ntask test {\n  input {\n    String in1\n    String
        in2\n    String in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo
        ~{in3}\n    }\n    output {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Required
        workflow input ''parseBatchFile.batchFile'' not specified"}],"message":"Workflow
        input processing failed"}],"end":"2025-04-09T17:39:03.135Z","start":"2025-04-09T17:39:03.120Z","id":"caac32bb-a81a-46a3-8b9c-216c04a3743e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-caac32bb-a81a-46a3-8b9c-216c04a3743e"},"submission":"2025-04-09T17:26:10.021Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Wed, 09 Apr 2025 17:39:11 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '2382'
    status:
      code: 200
      message: OK
version: 1
