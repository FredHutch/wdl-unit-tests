interactions:
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/0eb0d729-df69-4ab6-9bdf-9805cd6ada98/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowName":"test_nonstandard_outputs","workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:56:18.922Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T20:57:10.967Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\nworkflow test_nonstandard_outputs {\n    call generate_diverse_outputs\n    \n    output
        {\n        File special_chars = generate_diverse_outputs.file_special_chars\n        File
        no_extension = generate_diverse_outputs.file_no_extension\n        File nested_output
        = generate_diverse_outputs.nested_file\n        File symlink_file = generate_diverse_outputs.symlink_output\n        Array[File]
        glob_files = generate_diverse_outputs.pattern_files\n    }\n}\n\ntask generate_diverse_outputs
        {\n    command <<<\n        # File with special characters\n        echo \"test
        content\" > \"test@file#1.txt\"\n        \n        # File without extension\n        echo
        \"no extension\" > datafile\n        \n        # Nested directory output\n        mkdir
        -p nested/dir\n        echo \"nested content\" > nested/dir/test.txt\n        \n        #
        Create a symlink\n        echo \"original\" > original.txt\n        ln -s
        original.txt link.txt\n        \n        # Multiple pattern files\n        for
        i in {1..3}; do\n            echo \"pattern $i\" > \"pattern_$i.out\"\n        done\n    >>>\n\n    output
        {\n        File file_special_chars = \"test@file#1.txt\"\n        File file_no_extension
        = \"datafile\"\n        File nested_file = \"nested/dir/test.txt\"\n        File
        symlink_output = \"link.txt\"\n        Array[File] pattern_files = glob(\"pattern_*.out\")\n    }\n\n    runtime
        {\n        docker: \"ubuntu:noble-20241118.1\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{"test_nonstandard_outputs.symlink_file":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/test_nonstandard_outputs/0eb0d729-df69-4ab6-9bdf-9805cd6ada98/call-generate_diverse_outputs/execution/link.txt","test_nonstandard_outputs.nested_output":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/test_nonstandard_outputs/0eb0d729-df69-4ab6-9bdf-9805cd6ada98/call-generate_diverse_outputs/execution/nested/dir/test.txt","test_nonstandard_outputs.glob_files":["/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/test_nonstandard_outputs/0eb0d729-df69-4ab6-9bdf-9805cd6ada98/call-generate_diverse_outputs/execution/glob-4ded3a97b6654226f2cdd04e2711b93c/pattern_1.out","/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/test_nonstandard_outputs/0eb0d729-df69-4ab6-9bdf-9805cd6ada98/call-generate_diverse_outputs/execution/glob-4ded3a97b6654226f2cdd04e2711b93c/pattern_2.out","/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/test_nonstandard_outputs/0eb0d729-df69-4ab6-9bdf-9805cd6ada98/call-generate_diverse_outputs/execution/glob-4ded3a97b6654226f2cdd04e2711b93c/pattern_3.out"],"test_nonstandard_outputs.no_extension":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/test_nonstandard_outputs/0eb0d729-df69-4ab6-9bdf-9805cd6ada98/call-generate_diverse_outputs/execution/datafile","test_nonstandard_outputs.special_chars":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/test_nonstandard_outputs/0eb0d729-df69-4ab6-9bdf-9805cd6ada98/call-generate_diverse_outputs/execution/test@file#1.txt"},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/test_nonstandard_outputs/0eb0d729-df69-4ab6-9bdf-9805cd6ada98","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-02-27T20:57:10.967Z","start":"2025-02-27T20:56:18.922Z","id":"0eb0d729-df69-4ab6-9bdf-9805cd6ada98","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-0eb0d729-df69-4ab6-9bdf-9805cd6ada98"},"submission":"2025-02-27T20:56:18.213Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:35:52 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3987'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/72311262-888d-4b7b-b328-3a718df2a617/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:59:59.203Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T20:59:59.345Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\nworkflow ArrayOperations {\n    input {\n        # Input arrays for
        different tests\n        Array[String] strings\n        Array[String] additional_strings
        = []  # For testing array concatenation\n        Array[Array[String]] nested_arrays
        = []  # For testing nested arrays\n        Array[Int] numbers = [1, 2, 3,
        4, 5]  # Default integer array for numeric operations\n        Array[File]
        input_files = [] # Array of files to test file operations\n    }\n    \n    #
        Scatter operation to test processing of each element in an array\n    # Test
        empty arrays (original operation still works with empty input)\n    scatter
        (str in strings) {\n        call Uppercase { input: text = str }\n    }\n    \n    #
        Test array indexing (accessing first and last elements)\n    if (length(strings)
        > 0) {\n        call ValidateIndex { input: arr = strings }\n    }\n    \n    #
        Test array functions like sorting, length calculation, and flattening\n    call
        ArrayFunctions { \n        input: \n            arr = strings,\n            nested
        = nested_arrays\n    }\n    \n    # Test array concatenation and verify the
        combined length\n    Array[String] combined = flatten([strings, additional_strings])\n    call
        ArrayConcat {\n        input: \n            arr1 = strings,\n            arr2
        = additional_strings,\n            expected_length = length(combined)\n    }\n    \n    #
        Test integer array operations like summation and combining arrays\n    Array[Int]
        more_numbers = [6, 7, 8, 9, 10]  # Intermediate array declaration\n    call
        IntegerArrayOps {\n        input:\n            numbers = numbers,\n            additional_numbers
        = more_numbers\n    }\n\n    # Test file array operations like localization
        and content reading\n    if (length(input_files) > 0) {\n        call FileArrayOps
        {\n            input:\n                files = input_files\n        }\n    }\n    #
        Outputs to capture results of the tests\n    output {\n        Array[String]
        uppercased = Uppercase.out # Outputs from scatter task\n        Int? first_index
        = ValidateIndex.first_index # First index in string array\n        Int? last_index
        = ValidateIndex.last_index # Last index in string array\n        Array[String]
        sorted_array = ArrayFunctions.sorted # Sorted array\n        Array[Array[String]]
        processed_nested = ArrayFunctions.processed_nested # Processed nested array\n        Boolean
        concat_test_passed = ArrayConcat.test_passed # Result of concatenation test\n        Int
        array_length = ArrayFunctions.arr_length # Length of input array\n        Array[String]
        flattened = ArrayFunctions.flattened # Flattened nested arrays\n        #
        New outputs for integer array operations \n        Int sum_result = IntegerArrayOps.sum
        # Sum of integer array\n        Array[Int] combined_numbers = IntegerArrayOps.combined
        # Combined integer arrays\n        # New outputs for file array operations\n        Array[String]?
        file_contents = FileArrayOps.contents # Contents of files\n        Boolean?
        files_localized = FileArrayOps.localization_success # File localization status\n    }\n\n    parameter_meta
        {\n        # Descriptions for inputs\n        strings: \"Primary array of
        input strings\"\n        additional_strings: \"Secondary array for testing
        concatenation\"\n        nested_arrays: \"Array of arrays for testing nested
        array operations\"\n        numbers: \"Array of integers for testing numeric
        operations\"\n        input_files: \"Array of input files for testing file
        localization\"\n    }\n}\n\n# Task to convert string to uppercase (tests per-element
        processing)\ntask Uppercase {\n    input {\n        String text\n    }\n    \n    command
        <<<\n        echo \"~{text}\" | tr ''[:lower:]'' ''[:upper:]''\n    >>>\n    \n    output
        {\n        String out = read_string(stdout())\n    }\n    \n    runtime {\n        cpu:
        1\n        memory: \"1 GB\"\n    }\n}\n\n\n# Task to test indexing operations\ntask
        ValidateIndex {\n    input {\n        Array[String] arr\n    }\n    \n    command
        <<<\n        echo \"0\" > first_index.txt  # First index\n        echo \"~{length(arr)-1}\"
        > last_index.txt  # Last index\n    >>>\n    \n    output {\n        Int first_index
        = read_int(\"first_index.txt\")\n        Int last_index = read_int(\"last_index.txt\")\n    }\n    \n    runtime
        {\n        cpu: 1\n        memory: \"1 GB\"\n    }\n}\n\n# Task to test array
        functions\ntask ArrayFunctions {\n    input {\n        Array[String] arr\n        Array[Array[String]]
        nested\n    }\n    \n    command <<<\n        # Sort the input array using
        bash\n        echo \"~{sep=''\\n'' arr}\" | sort > sorted.txt\n        \n        #
        Get array length\n        echo \"~{length(arr)}\" > length.txt\n        \n        #
        Process nested arrays (flatten them)\n        echo \"~{sep=''\\n'' flatten(nested)}\"
        > flattened.txt\n    >>>\n    \n    output {\n        Array[String] sorted
        = read_lines(\"sorted.txt\")\n        Int arr_length = read_int(\"length.txt\")\n        Array[String]
        flattened = read_lines(\"flattened.txt\")\n        Array[Array[String]] processed_nested
        = nested  # Return the original nested array\n    }\n    \n    runtime {\n        cpu:
        1\n        memory: \"1 GB\"\n    }\n}\n\n# Task to test concatenation of two
        arrays\ntask ArrayConcat {\n    input {\n        Array[String] arr1\n        Array[String]
        arr2\n        Int expected_length\n    }\n    \n    command <<<\n        actual_length=$((
        ~{length(arr1)} + ~{length(arr2)} ))\n        if [ \"$actual_length\" -eq
        ~{expected_length} ]; then\n            echo \"true\"\n        else\n            echo
        \"false\"\n        fi\n    >>>\n    \n    output {\n        Boolean test_passed
        = read_boolean(stdout())\n    }\n    \n    runtime {\n        cpu: 1\n        memory:
        \"1 GB\"\n    }\n}\n\n# Task to test integer array operations\ntask IntegerArrayOps
        {\n    input {\n        Array[Int] numbers\n        Array[Int] additional_numbers\n    }\n    \n    command
        <<<\n        # Calculate sum of numbers to verify proper parsing\n        total=0\n        for
        num in ~{sep='' '' numbers}; do\n            total=$((total + num))\n        done\n        echo
        $total > sum.txt\n\n        # Combine arrays and write to file\n        echo
        \"~{sep=''\\n'' flatten([numbers, additional_numbers])}\" > combined.txt\n    >>>\n    \n    output
        {\n        Int sum = read_int(\"sum.txt\")\n        Array[Int] combined =
        read_lines(\"combined.txt\")\n    }\n    \n    runtime {\n        cpu: 1\n        memory:
        \"1 GB\"\n    }\n}\n\n# Task to test file array operations\ntask FileArrayOps
        {\n    input {\n        Array[File] files\n    }\n    \n    command <<<\n        #
        Test file localization by reading contents\n        for file in ~{sep='' ''
        files}; do\n            if [ -f \"$file\" ]; then\n                cat \"$file\"
        >> all_contents.txt\n                echo \"---\" >> all_contents.txt  # Separator
        between files\n            else\n                echo \"false\" > localization_success.txt\n                exit
        1\n            fi\n        done\n        echo \"true\" > localization_success.txt\n    >>>\n    \n    output
        {\n        Array[String] contents = read_lines(\"all_contents.txt\")\n        Boolean
        localization_success = read_boolean(\"localization_success.txt\")\n    }\n    \n    runtime
        {\n        cpu: 1\n        memory: \"1 GB\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Required
        workflow input ''ArrayOperations.strings'' not specified"}],"message":"Workflow
        input processing failed"}],"end":"2025-02-27T20:59:59.345Z","start":"2025-02-27T20:59:59.203Z","id":"72311262-888d-4b7b-b328-3a718df2a617","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-72311262-888d-4b7b-b328-3a718df2a617"},"submission":"2025-02-27T20:56:18.243Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:35:57 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '8300'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/da2c92b8-7265-426c-9626-6cb202b0b04c/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T21:01:39.312Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T21:01:39.342Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\nworkflow enhanced_map_test {\n    input {\n        # Original inputs\n        Array[String]
        samples\n        Map[String, String] sample_metadata\n        Map[String,
        Int] read_lengths\n\n        # New test inputs\n        Map[String, Map[String,
        String]] nested_map = {\n            \"patient1\": {\n                \"sample1\":
        \"normal\",\n                \"sample2\": \"tumor\"\n            },\n            \"patient2\":
        {\n                \"sample3\": \"normal\",\n                \"sample4\":
        \"tumor\"\n            }\n        }\n        # We need to provide keys as
        arrays since WDL 1.0 doesn''t have a keys() function\n        Array[String]
        patient_ids = [\"patient1\", \"patient2\"]\n    }\n\n    # Test nested map
        processing\n    scatter (patient_id in patient_ids) {\n        call process_nested_map
        {\n            input:\n                patient_id = patient_id,\n                patient_data
        = nested_map[patient_id],\n                # We need to provide the sample
        names explicitly\n                samples_for_patient = if patient_id == \"patient1\"
        then [\"sample1\", \"sample2\"] else [\"sample3\", \"sample4\"]\n        }\n    }\n\n    #
        Original sample processing with output map generation\n    scatter (sample
        in samples) {\n        call process_sample {\n            input:\n                sample_name
        = sample,\n                sample_type = sample_metadata[sample],\n                read_length
        = read_lengths[sample]\n        }\n    }\n\n    # Aggregate results into a
        map\n    call create_result_map {\n        input:\n            sample_names
        = samples,\n            processing_messages = process_sample.message\n    }\n\n    output
        {\n        Map[String, String] result_map = create_result_map.output_map\n        Array[String]
        nested_map_results = process_nested_map.message\n    }\n}\n\ntask process_nested_map
        {\n    input {\n        String patient_id\n        Map[String, String] patient_data\n        Array[String]
        samples_for_patient\n    }\n\n    # First get the first sample ID\n    String
        first_sample = samples_for_patient[0]\n    # Then use it to index the patient
        data\n    String sample_type = patient_data[first_sample]\n\n    command {\n        echo
        \"Processing patient ${patient_id} with sample type ${sample_type}\"\n        for
        sample in ${sep='' '' samples_for_patient}; do\n            echo \"Sample:
        $sample\"\n        done\n    }\n\n    output {\n        String message = read_string(stdout())\n    }\n\n    runtime
        {\n        docker: \"ubuntu:noble-20241118.1\"\n    }\n}\n\ntask process_sample
        {\n    input {\n        String sample_name\n        String sample_type\n        Int
        read_length\n    }\n\n    command <<<\n        echo \"Processing ~{sample_name}
        (~{sample_type}) with read length ~{read_length}\"\n    >>>\n\n    output
        {\n        String message = read_string(stdout())\n    }\n\n    runtime {\n        docker:
        \"ubuntu:noble-20241118.1\"\n    }\n}\n\ntask create_result_map {\n    input
        {\n        Array[String] sample_names\n        Array[String] processing_messages\n    }\n\n    command
        <<<\n        python <<CODE\n        samples = ''~{sep=\",\" sample_names}''.split('','')\n        messages
        = ''~{sep=\",\" processing_messages}''.split('','')\n        result = dict(zip(samples,
        messages))\n        with open(''output.txt'', ''w'') as f:\n            for
        sample, message in result.items():\n                f.write(f\"{sample}\\t{message}\\n\")\n        CODE\n    >>>\n\n    output
        {\n        Map[String, String] output_map = read_map(''output.txt'')\n    }\n\n    runtime
        {\n        docker: \"python:3.8-slim\"\n    }\n}","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Required
        workflow input ''enhanced_map_test.sample_metadata'' not specified"},{"causedBy":[],"message":"Required
        workflow input ''enhanced_map_test.read_lengths'' not specified"},{"causedBy":[],"message":"Required
        workflow input ''enhanced_map_test.samples'' not specified"}],"message":"Workflow
        input processing failed"}],"end":"2025-02-27T21:01:39.341Z","start":"2025-02-27T21:01:39.313Z","id":"da2c92b8-7265-426c-9626-6cb202b0b04c","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-da2c92b8-7265-426c-9626-6cb202b0b04c"},"submission":"2025-02-27T20:56:18.276Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:02 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '4807'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/2cb5f50f-ab2d-4640-94a1-fc023782e862/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T20:56:59.000Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:56:58.987Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\nworkflow jsonTaskOrderTest {\n  input {\n    String input_json  # JSON
        string used as input for both tasks\n  }\n\n  call Task1 { input: input_json
        = input_json }\n  call Task2 { input: input_json = input_json, previous_output
        = Task1.output_file }\n\n  output {\n    File task1_output = Task1.output_file\n    File
        task2_output = Task2.output_file\n  }\n}\n\ntask Task1 {\n  input {\n    String
        input_json\n  }\n  \n  command <<<\n    echo \"Processing JSON in Task1: ~{input_json}\"
        > task1_output.txt\n    echo \"Task1 completed\" >> task1_output.txt\n  >>>\n  \n  output
        {\n    File output_file = \"task1_output.txt\"\n  }\n  \n  runtime {\n    cpu:
        1\n    memory: \"2G\"\n  }\n}\n\ntask Task2 {\n  input {\n    String input_json\n    File
        previous_output\n  }\n  \n  command <<<\n    echo \"Processing JSON in Task2:
        ~{input_json}\" > task2_output.txt\n    echo \"Task2 completed after Task1\"
        >> task2_output.txt\n    cat ~{previous_output} >> task2_output.txt\n  >>>\n  \n  output
        {\n    File output_file = \"task2_output.txt\"\n  }\n  \n  runtime {\n    cpu:
        1\n    memory: \"2G\"\n  }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Required
        workflow input ''jsonTaskOrderTest.input_json'' not specified"}],"message":"Workflow
        input processing failed"}],"end":"2025-02-27T20:56:59.000Z","start":"2025-02-27T20:56:58.987Z","id":"2cb5f50f-ab2d-4640-94a1-fc023782e862","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-2cb5f50f-ab2d-4640-94a1-fc023782e862"},"submission":"2025-02-27T20:56:18.304Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:07 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '2130'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/4dc14af1-14e0-4c43-a9aa-6193942b0043/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowName":"globNonmatching","workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T20:59:13.186Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:58:19.095Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\nworkflow globNonmatching {\n    call create_files\n    output {\n        Array[File]
        unmatched_files = create_files.unmatched_files\n    }\n}\n\ntask create_files
        {\n    command <<<\n        echo \"Test file\" > test.txt\n    >>>\n    output
        {\n        Array[File] unmatched_files = glob(\"*.log\")\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{"globNonmatching.unmatched_files":[]},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/globNonmatching/4dc14af1-14e0-4c43-a9aa-6193942b0043","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-02-27T20:59:13.186Z","start":"2025-02-27T20:58:19.095Z","id":"4dc14af1-14e0-4c43-a9aa-6193942b0043","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-4dc14af1-14e0-4c43-a9aa-6193942b0043"},"submission":"2025-02-27T20:56:18.332Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1367'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/69aed49e-6ee6-4ff5-b90f-c414edc2ab67/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowName":"basicGlobTest","workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T21:00:32.256Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:59:39.182Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\nworkflow basicGlobTest {\n    call create_files\n    output {\n        Array[File]
        matched_files = create_files.txt_files\n    }\n}\n\ntask create_files {\n    command
        <<<\n        echo \"File 1\" > file1.txt\n        echo \"File 2\" > file2.txt\n    >>>\n    output
        {\n        Array[File] txt_files = glob(\"*.txt\")\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{"basicGlobTest.matched_files":["/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/basicGlobTest/69aed49e-6ee6-4ff5-b90f-c414edc2ab67/call-create_files/execution/glob-ef5df339533c1334f081dc8cc75ee4f3/file1.txt","/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/basicGlobTest/69aed49e-6ee6-4ff5-b90f-c414edc2ab67/call-create_files/execution/glob-ef5df339533c1334f081dc8cc75ee4f3/file2.txt"]},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/basicGlobTest/69aed49e-6ee6-4ff5-b90f-c414edc2ab67","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-02-27T21:00:32.256Z","start":"2025-02-27T20:59:39.183Z","id":"69aed49e-6ee6-4ff5-b90f-c414edc2ab67","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-69aed49e-6ee6-4ff5-b90f-c414edc2ab67"},"submission":"2025-02-27T20:56:18.360Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1741'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/e1f2355b-862d-4e10-9b0a-0c860fd127bb/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowName":"HelloHostname","workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T21:01:59.333Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T21:02:51.376Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n## This is a test workflow that returns the hostname of the node \n##
        the job is submitted to as a test for the Gizmo backend. \n\n#### WORKFLOW
        DEFINITION\n\nworkflow HelloHostname {\n  call Hostname {\n  }\n\n  output
        {\n    File stdout = Hostname.out\n  }\n\n  parameter_meta {\n    stdout:
        \"hostname of the node the job was submitted to\"\n  }\n}\n\n#### TASK DEFINITIONS\n\ntask
        Hostname {\n  command <<<\n    echo $(hostname)\n  >>>\n\n  output {\n    File
        out = stdout()\n  }\n  \n  runtime {\n    cpu: 1\n    memory: \"1 GB\"\n  }\n\n  parameter_meta
        {\n    out: \"hostname of the node the job was submitted to\"\n  }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{"HelloHostname.stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/HelloHostname/e1f2355b-862d-4e10-9b0a-0c860fd127bb/call-Hostname/execution/stdout"},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/HelloHostname/e1f2355b-862d-4e10-9b0a-0c860fd127bb","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-02-27T21:02:51.376Z","start":"2025-02-27T21:01:59.334Z","id":"e1f2355b-862d-4e10-9b0a-0c860fd127bb","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-e1f2355b-862d-4e10-9b0a-0c860fd127bb"},"submission":"2025-02-27T20:56:18.389Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:23 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1805'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/35b78f55-f6bf-4dee-833a-8686e9509a0f/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:57:39.042Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T20:57:39.224Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# Define the structure for sampleDetails\nstruct sampleDetails {\n    String
        experimentType\n    String prepMethod\n    String tissueType\n}\n\n# Define
        the main structure for the single sample\nstruct singleSample {\n    String
        sampleName\n    String aboutSample\n    String sampleDescription\n    sampleDetails
        details  # Use the sampleDetails struct here\n}\n\nworkflow testNestedJsonArray
        {\n  input {\n    String cellNumber\n    Array[singleSample] batchOfSamples  #
        Array of objects representing each sample\n  }\n\n  scatter (sample in batchOfSamples)
        {\n    call processSample {\n      input:\n        sample = sample,\n        base_file_name
        = sample.sampleName \n    }\n  }\n\n  output {\n    # Collect all the fields
        together from each sample into one list\n    Array[File] result_allSampleInfo
        = processSample.allSampleInfo\n  }\n}\n\ntask processSample {\n  input {\n    singleSample
        sample  # Use singleSample type, not Object\n    String base_file_name\n  }\n\n  command
        <<<\n    # Format the sample information as a single string\n    allSampleInfo=\"~{sample.sampleName}
        | ~{sample.aboutSample} | ~{sample.sampleDescription} | ~{sample.details.experimentType}
        | ~{sample.details.prepMethod} | ~{sample.details.tissueType}\"\n    \n    #
        Output the concatenated sample info to a file\n    echo \"${allSampleInfo}\"
        > ~{base_file_name}.allSampleInfo.txt\n  >>>\n\n  output {\n    # Read all
        sample info from the file and output it as an Array of Strings\n    File allSampleInfo
        = \"${base_file_name}.allSampleInfo.txt\"\n  }\n\n  runtime {\n    docker:
        \"ubuntu:20.04\"\n  }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Required
        workflow input ''testNestedJsonArray.batchOfSamples'' not specified"},{"causedBy":[],"message":"Required
        workflow input ''testNestedJsonArray.cellNumber'' not specified"}],"message":"Workflow
        input processing failed"}],"end":"2025-02-27T20:57:39.224Z","start":"2025-02-27T20:57:39.043Z","id":"35b78f55-f6bf-4dee-833a-8686e9509a0f","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-35b78f55-f6bf-4dee-833a-8686e9509a0f"},"submission":"2025-02-27T20:56:18.418Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:28 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '2731'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/a5b021e0-a849-454a-82b5-b3881af1f315/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T21:00:19.221Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T21:00:32.307Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n## This workflow demonstrates the usage of conditional statements in
        WDL\n## by selectively processing samples based on their properties\n\nstruct
        SampleInfo {\n    String name\n    Float quality_score\n    String type\n}\n\nworkflow
        conditional_example {\n    input {\n        Array[SampleInfo] samples\n        Float
        quality_threshold\n    }\n\n    # Demonstrate if statement in scatter\n    scatter
        (sample in samples) {\n        if (sample.quality_score >= quality_threshold)
        {\n            call process_high_quality {\n                input:\n                    sample
        = sample\n            }\n        }\n    }\n\n    # Create string arrays for
        the QC report\n    scatter (sample in samples) {\n        String sample_line
        = \"~{sample.name},~{sample.type},~{sample.quality_score}\"\n    }\n\n    #
        Demonstrate single conditional task\n    call run_qc_report {\n        input:\n            sample_lines
        = sample_line\n    }\n\n    # Calculate number of high quality samples\n    Int
        num_high_quality = length(select_all(process_high_quality.message))\n\n    #
        Demonstrate separate conditional blocks (WDL 1.0 approach instead of if/else)\n    Boolean
        has_multiple_samples = num_high_quality > 1\n    \n    if (has_multiple_samples)
        {\n        call summarize {\n            input:\n                messages
        = select_all(process_high_quality.message),\n                report = \"Multiple
        high-quality samples processed\"\n        }\n    }\n\n    if (!has_multiple_samples)
        {\n        call summarize as summarize_few {\n            input:\n                messages
        = select_all(process_high_quality.message),\n                report = \"Few
        or no high-quality samples found\"\n        }\n    }\n\n    output {\n        String
        final_summary = select_first([summarize.summary, summarize_few.summary])\n        File
        qc_report = run_qc_report.report_csv\n    }\n}\n\ntask process_high_quality
        {\n    input {\n        SampleInfo sample\n    }\n\n    command <<<\n        echo
        \"Processing high quality ~{sample.type} sample ~{sample.name} (Q=~{sample.quality_score})\"\n    >>>\n\n    output
        {\n        String message = read_string(stdout())\n    }\n\n    runtime {\n        docker:
        \"ubuntu:noble-20241118.1\"\n    }\n}\n\ntask run_qc_report {\n    input {\n        Array[String]
        sample_lines\n    }\n\n    command <<<\n        echo \"Quality Score Summary:\"\n        echo
        \"Sample Name,Type,Quality Score\" > report.csv\n        ~{sep=\"\\n\" sample_lines}
        >> report.csv\n        cat report.csv\n    >>>\n\n    output {\n        String
        report = read_string(stdout())\n        File report_csv = \"report.csv\"\n    }\n\n    runtime
        {\n        docker: \"ubuntu:noble-20241118.1\"\n    }\n}\n\ntask summarize
        {\n    input {\n        Array[String] messages\n        String report\n    }\n\n    command
        <<<\n        echo \"~{report}\"\n        echo \"Number of samples processed:
        ~{length(messages)}\"\n    >>>\n\n    output {\n        String summary = read_string(stdout())\n    }\n\n    runtime
        {\n        docker: \"ubuntu:noble-20241118.1\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Required
        workflow input ''conditional_example.quality_threshold'' not specified"},{"causedBy":[],"message":"Required
        workflow input ''conditional_example.samples'' not specified"}],"message":"Workflow
        input processing failed"}],"end":"2025-02-27T21:00:32.306Z","start":"2025-02-27T21:00:19.222Z","id":"a5b021e0-a849-454a-82b5-b3881af1f315","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-a5b021e0-a849-454a-82b5-b3881af1f315"},"submission":"2025-02-27T20:56:18.447Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:33 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '4186'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/6619768f-6029-4753-ae35-0cece541db9e/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T20:58:59.303Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:58:59.141Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n# This workflow takes a tab separated file where each row is a set of
        data to be used in each \n# of the independent scattered task series that
        you have as your workflow process.  This file \n# will, for example, have
        column names `sampleName`, `bamLocation`, and `bedlocation`.  This\n# allows
        you to know that regardless of the order of the columns in your batch file,
        the correct\n# inputs will be used for the tasks you define.  \nworkflow parseBatchFile
        {\n  input {\n  File batchFile\n  }\n    Array[Object] batchInfo = read_objects(batchFile)\n  scatter
        (job in batchInfo){\n    String sampleName = job.sampleName\n    File bamFile
        = job.bamLocation\n    File bedFile = job.bedLocation\n\n    ## INSERT YOUR
        WORKFLOW TO RUN PER LINE IN YOUR BATCH FILE HERE!!!!\n    call test {\n        input:
        in1=sampleName, in2=bamFile, in3=bedFile\n    }\n\n  }  # End Scatter over
        the batch file\n# Outputs that will be retained when execution is complete\n  output
        {\n    Array[File] outputArray = test.item_out\n    }\n# End workflow\n}\n\n####
        TASK DEFINITIONS\n# echo some text to stdout, treats files as strings just
        to echo them as a dummy example\ntask test {\n  input {\n    String in1\n    String
        in2\n    String in3\n  }\n    command {\n    echo ~{in1}\n    echo ~{in2}\n    echo
        ~{in3}\n    }\n    output {\n        File item_out = stdout()\n    }\n}","root":"","options":"{\n\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Required
        workflow input ''parseBatchFile.batchFile'' not specified"}],"message":"Workflow
        input processing failed"}],"end":"2025-02-27T20:58:59.302Z","start":"2025-02-27T20:58:59.142Z","id":"6619768f-6029-4753-ae35-0cece541db9e","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-6619768f-6029-4753-ae35-0cece541db9e"},"submission":"2025-02-27T20:56:18.481Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:38 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '2267'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/3173699d-0507-4129-b006-72062ac4c38c/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowName":"globSubdir","workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:57:19.014Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T20:58:04.948Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\nworkflow globSubdir {\n    call create_nested_files\n    output {\n        Array[File]
        matched_files = flatten([create_nested_files.matched_files_top, create_nested_files.matched_files_nested])\n    }\n}\n\ntask
        create_nested_files {\n    command <<<\n        mkdir -p subdir/nested\n        echo
        \"Hello\" > subdir/nested/file1.txt\n        echo \"World\" > subdir/file2.txt\n    >>>\n    output
        {\n        Array[File] matched_files_top = glob(\"subdir/*.txt\")\n        Array[File]
        matched_files_nested = glob(\"subdir/**/*.txt\")\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{"globSubdir.matched_files":["/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/globSubdir/3173699d-0507-4129-b006-72062ac4c38c/call-create_nested_files/execution/glob-ee3a9c1c6860f417d1e9ff1a72d2b62d/file2.txt","/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/globSubdir/3173699d-0507-4129-b006-72062ac4c38c/call-create_nested_files/execution/glob-4c0cd9dc6b12aa01233bbc214341aae1/file1.txt"]},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/globSubdir/3173699d-0507-4129-b006-72062ac4c38c","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-02-27T20:58:04.948Z","start":"2025-02-27T20:57:19.014Z","id":"3173699d-0507-4129-b006-72062ac4c38c","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-3173699d-0507-4129-b006-72062ac4c38c"},"submission":"2025-02-27T20:56:18.514Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:43 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1953'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/68267bc5-48ad-4170-b263-dccabf98caa7/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:59:19.162Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T20:59:19.171Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n## This is a test workflow that fails against womtool.\n## From https://github.com/broadinstitute/cromwell\n\n####
        WORKFLOW DEFINITION\n\nworkflow oops {\n  call oopsie\n}\n\n#### TASK DEFINITIONS\n\ntask
        oopsie {\n  input {\n    String str\n  }\n  command { echo ${str} }\n  runtime
        { docker: docker_image }\n}\n","root":"","options":"{\n\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Failed
        to process task definition ''oopsie'' (reason 1 of 1): Cannot lookup value
        ''docker_image'', it is never declared. Available values are: [''str'']"}],"message":"Workflow
        input processing failed"}],"end":"2025-02-27T20:59:19.171Z","start":"2025-02-27T20:59:19.163Z","id":"68267bc5-48ad-4170-b263-dccabf98caa7","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-68267bc5-48ad-4170-b263-dccabf98caa7"},"submission":"2025-02-27T20:56:18.543Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:48 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1307'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/602a2133-9800-400c-b18d-3c538497d84a/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowName":"emptyGlobTest","workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:58:39.121Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T20:59:27.097Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\nworkflow emptyGlobTest {\n    call create_empty_directory\n\n    output
        {\n        Array[File] no_files = create_empty_directory.no_files\n    }\n}\n\ntask
        create_empty_directory {\n    command {\n        mkdir empty_dir\n    }\n    output
        {\n        Array[File] no_files = glob(\"empty_dir/*.txt\")\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{"emptyGlobTest.no_files":[]},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/emptyGlobTest/602a2133-9800-400c-b18d-3c538497d84a","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-02-27T20:59:27.097Z","start":"2025-02-27T20:58:39.121Z","id":"602a2133-9800-400c-b18d-3c538497d84a","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-602a2133-9800-400c-b18d-3c538497d84a"},"submission":"2025-02-27T20:56:18.570Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:53 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1355'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/acd2ff7b-4387-4f16-b3c8-7727291b37d9/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T21:00:39.244Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T21:00:47.177Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\nworkflow WildcardsandConditions {\n    input {\n        String prefix  #
        Required input for the file prefix (no default value)\n    }\n\n    call wildcard_and_conditions_test
        {\n        input:\n            prefix = prefix  # Explicitly pass the workflow
        input to the task\n    }\n\n    output {\n        Array[File] txt_files =
        wildcard_and_conditions_test.txt_files\n        String conditional_result
        = wildcard_and_conditions_test.conditional_output\n    }\n}\n\ntask wildcard_and_conditions_test
        {\n    input {\n        String prefix  # Required input for file creation\n        Boolean
        create_extra_file = true  # Default value for conditional logic\n    }\n\n    command
        <<<\n        # Create multiple .txt files to test wildcard resolution\n        for
        i in {1..3}; do\n            echo \"File content $i\" > \"~{prefix}_$i.txt\"\n        done\n\n        #
        Create an extra file conditionally\n        if [[ ~{create_extra_file} ==
        \"true\" ]]; then\n            echo \"Extra file content\" > ~{prefix}_extra.txt\n        fi\n\n        #
        Parse inputs directly in the command\n        echo \"Parsed prefix: ~{prefix}\"
        > parsed_output.txt\n    >>>\n\n    output {\n        Array[File] txt_files
        = glob(\"*.txt\")  # Test wildcard resolution\n        String conditional_output
        = read_string(\"parsed_output.txt\")  # Verify input parsing\n    }\n\n    runtime
        {\n        docker: \"ubuntu:20.04\"\n    }\n}\n","root":"","options":"{\n\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Required
        workflow input ''WildcardsandConditions.prefix'' not specified"}],"message":"Workflow
        input processing failed"}],"end":"2025-02-27T21:00:47.177Z","start":"2025-02-27T21:00:39.244Z","id":"acd2ff7b-4387-4f16-b3c8-7727291b37d9","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-acd2ff7b-4387-4f16-b3c8-7727291b37d9"},"submission":"2025-02-27T20:56:18.600Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:36:58 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '2328'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/189a7398-a500-44af-bf6d-411c5b1bf2e0/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:56:38.942Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T20:56:38.959Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n## This workflow demonstrates advanced struct features in WDL including:\n##
        1. Optional fields\n## 2. Nested structs\n## 3. Default values (handled in
        the workflow)\n\n#### STRUCT DEFINITIONS\n\n# Nested struct for sequencing
        metadata\nstruct SequencingInfo {\n    String platform\n    String? flowcell_id\n    Int?
        lane_number\n}\n\n# Nested struct for quality metrics\nstruct QualityMetrics
        {\n    Float quality_score\n    Float? gc_content\n    Int? duplicate_rate\n}\n\n#
        Main struct with nested structures and optional fields\nstruct SampleInfo
        {\n    String name\n    String? type\n    Int? read_length\n    String? library_prep\n    SequencingInfo
        sequencing\n    QualityMetrics metrics\n}\n\n#### WORKFLOW DEFINITION\n\nworkflow
        struct_example {\n    input {\n        Array[SampleInfo] sample_information\n    }\n\n    scatter
        (sample_info in sample_information) {\n        SampleInfo processed_sample
        = object {\n            name: sample_info.name,\n            type: select_first([sample_info.type,
        \"normal\"]),\n            read_length: select_first([sample_info.read_length,
        100]),\n            library_prep: sample_info.library_prep,\n            sequencing:
        sample_info.sequencing,\n            metrics: sample_info.metrics\n        }\n\n        call
        process_sample {\n            input:\n                sample = processed_sample\n        }\n    }\n}\n\n####
        TASK DEFINITIONS\n\ntask process_sample {\n    input {\n        SampleInfo
        sample\n    }\n\n    command <<<\n        echo \"Processing ~{sample.name}
        (~{sample.type})\"\n        echo \"Read Length: ~{sample.read_length}\"\n        echo
        \"Sequencing Platform: ~{sample.sequencing.platform}\"\n        echo \"Flowcell
        ID: ~{select_first([sample.sequencing.flowcell_id, ''N/A''])}\"\n        echo
        \"Lane Number: ~{select_first([sample.sequencing.lane_number, -1])}\"\n        echo
        \"Quality Score: ~{sample.metrics.quality_score}\"\n        echo \"GC Content:
        ~{select_first([sample.metrics.gc_content, 0])}\"\n        echo \"Duplicate
        Rate: ~{select_first([sample.metrics.duplicate_rate, 0])}%\"\n        echo
        \"Library Prep: ~{select_first([sample.library_prep, ''Standard''])}\"\n    >>>\n\n    output
        {\n        String message = read_string(stdout())\n    }\n\n    runtime {\n        docker:
        \"ubuntu:noble-20241118.1\"\n    }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{},"actualWorkflowLanguage":"WDL","status":"Failed","failures":[{"causedBy":[{"causedBy":[],"message":"Required
        workflow input ''struct_example.sample_information'' not specified"}],"message":"Workflow
        input processing failed"}],"end":"2025-02-27T20:56:38.959Z","start":"2025-02-27T20:56:38.942Z","id":"189a7398-a500-44af-bf6d-411c5b1bf2e0","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-189a7398-a500-44af-bf6d-411c5b1bf2e0"},"submission":"2025-02-27T20:56:18.625Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:37:03 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '3341'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/d0b682ad-1512-4c1f-8ec0-cc13184184a4/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowName":"testFileoperations","workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T21:01:33.596Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T21:01:19.292Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\nworkflow testFileoperations {\n    call file_operations\n\n    output
        {\n        File created_file1 = file_operations.created_file1\n        File
        moved_file = file_operations.moved_file\n        File renamed_file = file_operations.renamed_file\n    }\n}\n\ntask
        file_operations {\n    command <<<\n        # Create three different files\n        echo
        \"This is the first created file.\" > file1.txt\n        echo \"This is the
        second file that will be moved.\" > file2.txt\n        echo \"This is the
        third file that will be renamed.\" > file3.txt\n        \n        # Move the
        second file to a new directory\n        mkdir -p output_dir\n        mv file2.txt
        output_dir/\n        \n        # Rename the third file\n        mv file3.txt
        file3_renamed.txt\n    >>>\n\n    output {\n        # Output the actual existing
        files\n        File created_file1 = \"file1.txt\"                  # The first
        file remains unchanged\n        File moved_file = \"output_dir/file2.txt\"          #
        The second file after being moved\n        File renamed_file = \"file3_renamed.txt\"           #
        The third file after being renamed\n    }\n\n    runtime {\n        docker:
        \"ubuntu:20.04\"\n    }\n}\n","root":"","options":"{\n\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{"testFileoperations.moved_file":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/testFileoperations/d0b682ad-1512-4c1f-8ec0-cc13184184a4/call-file_operations/cacheCopy/execution/output_dir/file2.txt","testFileoperations.renamed_file":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/testFileoperations/d0b682ad-1512-4c1f-8ec0-cc13184184a4/call-file_operations/cacheCopy/execution/file3_renamed.txt","testFileoperations.created_file1":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/testFileoperations/d0b682ad-1512-4c1f-8ec0-cc13184184a4/call-file_operations/cacheCopy/execution/file1.txt"},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/testFileoperations/d0b682ad-1512-4c1f-8ec0-cc13184184a4","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-02-27T21:01:33.596Z","start":"2025-02-27T21:01:19.293Z","id":"d0b682ad-1512-4c1f-8ec0-cc13184184a4","inputs":{},"labels":{"cromwell-workflow-id":"cromwell-d0b682ad-1512-4c1f-8ec0-cc13184184a4"},"submission":"2025-02-27T20:56:18.653Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:37:08 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '2701'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/3ae578ce-4376-4264-853e-755f6d77425a/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowName":"basicTaskTest","workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T20:57:59.063Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T20:58:14.547Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n\n# The basicTaskTest workflow calls a task named simpleTask, which takes
        a string input and writes it to a file called output.txt. It demonstrates
        a basic execution of a task with file output.\n\n# This tests basic task execution,
        input handling, and file output functionality. It ensures that a task can
        successfully take an input and generate an output.\n\nworkflow basicTaskTest
        {\n  input {\n    String text = \"Hello, World!\"\n  }\n\n  call simpleTask
        {\n    input:\n      message = text\n  }\n}\n\ntask simpleTask {\n  input
        {\n    String message\n  }\n\n  command <<<\n    echo \"${message}\" > output.txt\n    >>>\n\n  output
        {\n    File outputFile = \"output.txt\"\n  }\n\n  runtime {\n    docker: \"ubuntu:20.04\"\n  }\n}\n","root":"","options":"{\n\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{"basicTaskTest.simpleTask.outputFile":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/basicTaskTest/3ae578ce-4376-4264-853e-755f6d77425a/call-simpleTask/cacheCopy/execution/output.txt"},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/basicTaskTest/3ae578ce-4376-4264-853e-755f6d77425a","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-02-27T20:58:14.547Z","start":"2025-02-27T20:57:59.064Z","id":"3ae578ce-4376-4264-853e-755f6d77425a","inputs":{"text":"Hello,
        World!"},"labels":{"cromwell-workflow-id":"cromwell-3ae578ce-4376-4264-853e-755f6d77425a"},"submission":"2025-02-27T20:56:18.682Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:37:13 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '1847'
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - gizmok92.fhcrc.org:38409
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://gizmok92.fhcrc.org:38409/api/workflows/v1/c6ef8aa3-e0c7-4d9e-ba7c-673249b32ccd/metadata?expandSubWorkflows=false&excludeKey=calls
  response:
    body:
      string: '{"workflowName":"HelloDockerHostname","workflowProcessingEvents":[{"cromwellId":"cromid-2f25be4","description":"PickedUp","timestamp":"2025-02-27T21:00:59.272Z","cromwellVersion":"87"},{"cromwellId":"cromid-2f25be4","description":"Finished","timestamp":"2025-02-27T21:04:14.511Z","cromwellVersion":"87"}],"actualWorkflowLanguageVersion":"1.0","submittedFiles":{"workflow":"version
        1.0\n## This is a test workflow that returns the Docker image name and tag\n##
        and measures execution time of the Hostname task.\n\n#### WORKFLOW DEFINITION\n\nworkflow
        HelloDockerHostname {\n  input {\n    String docker_image = \"ubuntu:20.04\"  #
        Default value but can be overridden\n  }\n\n  call GetStartTime\n\n  call
        Hostname {\n    input:\n      expected_image = docker_image,\n      start_time
        = GetStartTime.timestamp  # Add dependency on start time\n  }\n\n  call GetEndTime
        {\n    input:\n      hostname_done = Hostname.out  # Add dependency on Hostname
        completion\n  }\n\n  call ValidateExecutionTime {\n    input:\n      start_time
        = GetStartTime.timestamp,\n      end_time = GetEndTime.timestamp\n  }\n\n  output
        {\n    File stdout = Hostname.out\n    Float execution_time_seconds = ValidateExecutionTime.duration_seconds\n    Boolean
        within_time_limit = ValidateExecutionTime.within_limit\n  }\n\n  parameter_meta
        {\n    docker_image: \"Docker image to run the task in (e.g. ubuntu:latest)\"\n  }\n}\n\n####
        TASK DEFINITIONS\n\ntask GetStartTime {\n  command <<<\n    date +%s.%N\n  >>>\n\n  output
        {\n    Float timestamp = read_float(stdout())\n  }\n\n  runtime {\n    docker:
        \"ubuntu:20.04\"\n    cpu: 1\n    memory: \"1 GB\"\n  }\n}\n\ntask GetEndTime
        {\n  input {\n    File hostname_done  # Add dependency on Hostname completion\n  }\n\n  command
        <<<\n    date +%s.%N\n  >>>\n\n  output {\n    Float timestamp = read_float(stdout())\n  }\n\n  runtime
        {\n    docker: \"ubuntu:20.04\"\n    cpu: 1\n    memory: \"1 GB\"\n  }\n}\n\ntask
        ValidateExecutionTime {\n  input {\n    Float start_time\n    Float end_time\n  }\n\n  command
        <<<\n    # Calculate duration using awk for floating point arithmetic\n    duration=$(awk
        \"BEGIN {print ~{end_time} - ~{start_time}}\")\n    echo \"$duration\" > duration.txt\n    \n    #
        Check if duration is less than 120 seconds (2 minutes)\n    awk -v dur=\"$duration\"
        ''BEGIN {if (dur < 120) exit 0; exit 1}''\n    if [ $? -eq 0 ]; then\n      echo
        \"true\" > within_limit.txt\n    else\n      echo \"false\" > within_limit.txt\n    fi\n  >>>\n\n  output
        {\n    Float duration_seconds = read_float(\"duration.txt\")\n    Boolean
        within_limit = read_boolean(\"within_limit.txt\")\n  }\n\n  runtime {\n    docker:
        \"ubuntu:20.04\"\n    cpu: 1\n    memory: \"1 GB\"\n  }\n}\n\ntask Hostname
        {\n  input {\n    String expected_image\n    Float start_time  # Add start_time
        as input to create dependency\n  }\n\n  command <<<\n    # Split expected
        image into name and tag\n    EXPECTED_IMAGE_NAME=$(echo \"~{expected_image}\"
        | cut -d'':'' -f1)\n    EXPECTED_TAG=$(echo \"~{expected_image}\" | cut -d'':''
        -f2)\n\n    # Get current image info\n    CURRENT_IMAGE=$(grep \"ID=\" /etc/os-release
        | head -n1 | cut -d''='' -f2)\n    CURRENT_VERSION=$(grep \"VERSION_ID=\"
        /etc/os-release | cut -d''\"'' -f2)\n\n    # Compare image name\n    if [[
        \"$CURRENT_IMAGE\" != \"$EXPECTED_IMAGE_NAME\" ]]; then\n      echo \"Error:
        Expected Docker image $EXPECTED_IMAGE_NAME but got: $CURRENT_IMAGE\"\n      exit
        1\n    fi\n\n    # Compare version/tag\n    if [[ \"$CURRENT_VERSION\" !=
        \"$EXPECTED_TAG\" ]]; then\n      echo \"Error: Expected version $EXPECTED_TAG
        but got: $CURRENT_VERSION\"\n      exit 1\n    fi\n\n    echo \"Verified Docker
        Image: $CURRENT_IMAGE:$CURRENT_VERSION\"\n    echo \"Expected Image: ~{expected_image}\"\n    echo
        \"Hostname: $(hostname)\"\n  >>>\n\n  output {\n    File out = stdout()\n  }\n\n  runtime
        {\n    cpu: 1\n    memory: \"1 GB\"\n    docker: \"~{expected_image}\"\n  }\n\n  parameter_meta
        {\n    expected_image: \"Docker image that should be running this task\"\n  }\n}\n","root":"","options":"{\n  \"read_from_cache\":
        false,\n  \"workflow_failure_mode\": \"ContinueWhilePossible\",\n  \"write_to_cache\":
        false\n}","inputs":"{}","workflowUrl":"","labels":"{}"},"calls":{},"outputs":{"HelloDockerHostname.execution_time_seconds":87.2414,"HelloDockerHostname.within_time_limit":true,"HelloDockerHostname.stdout":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/HelloDockerHostname/c6ef8aa3-e0c7-4d9e-ba7c-673249b32ccd/call-Hostname/execution/stdout"},"workflowRoot":"/redacted/_DaSL/user/svc_proof_test/cromwell-scratch/HelloDockerHostname/c6ef8aa3-e0c7-4d9e-ba7c-673249b32ccd","actualWorkflowLanguage":"WDL","status":"Succeeded","end":"2025-02-27T21:04:14.511Z","start":"2025-02-27T21:00:59.273Z","id":"c6ef8aa3-e0c7-4d9e-ba7c-673249b32ccd","inputs":{"docker_image":"ubuntu:20.04"},"labels":{"cromwell-workflow-id":"cromwell-c6ef8aa3-e0c7-4d9e-ba7c-673249b32ccd"},"submission":"2025-02-27T20:56:18.713Z"}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Thu, 27 Feb 2025 21:37:18 GMT
      Server:
      - nginx/1.25.3
      Transfer-Encoding:
      - chunked
      content-length:
      - '4958'
    status:
      code: 200
      message: OK
version: 1
